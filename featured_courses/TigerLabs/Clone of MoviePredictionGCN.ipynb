{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Clone of MoviePredictionGCN.ipynb","provenance":[{"file_id":"11tcL4KXXwY__TmUUTjOf6InFQMC-VsG6","timestamp":1579748052097}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"14Qxtu0mPcat","colab_type":"text"},"source":["<p><img alt=\"TigerGraph logo\" height=\"45px\" src=\"https://blobscdn.gitbook.com/v0/b/gitbook-28427.appspot.com/o/spaces%2F-LHvjxIN4__6bA0T-QmU%2Favatar.png?generation=1532158270801864&amp;alt=media\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n","\n","# Graph Convolutional Neural Networks for Movie Recommendation\n","------\n","## Introduction\n","This notebook walks through a basic example of using a graph convolutional neural network (GCN) for recommendation. The data is collected from a TigerGraph database using a Python package [pyTigerGraph](https://github.com/parkererickson/pyTigerGraph). Data collected is then pushed through a GCN to output predictions about a person's viewing preferences. This example does makes a couple oversimplifications that will be pointed out, mainly in the assumptions made surrounding a person's preferences.\n"]},{"cell_type":"markdown","metadata":{"id":"6Kucj9I3UoWl","colab_type":"text"},"source":["Collab Notebook [Original Version ](https://colab.research.google.com/drive/11tcL4KXXwY__TmUUTjOf6InFQMC-VsG6)"]},{"cell_type":"markdown","metadata":{"id":"DlYp_HBZMMZg","colab_type":"text"},"source":["## Install Queries on TigerGraph Server\n","You need to create and install two queries on the TigerGraph server; one named userRatings and another called movieLinks.\n","\n","```\n","CREATE QUERY userRatings(VERTEX<USER> user) FOR GRAPH Recommender { \n","  /* movieID | userID | userRating | term | termRating */\n","  SumAccum<float> @rating;\n","\t\n","\tsrc = {user};\n","  \n","\tS1 = SELECT tgt FROM src:s -(rate:e)-> MOVIE:tgt\n","       ACCUM tgt.@rating += e.rating;\n","\n","  PRINT S1[S1.movie_id as movieID, S1.name as movieTitle, S1.@rating as userRating];\n","}\n","```\n","\n","```\n","CREATE QUERY movieLinks() FOR GRAPH Recommender SYNTAX v2{ \n","\tTYPEDEF TUPLE <STRING src, STRING dest> TUPLE_RECORD;\n","\tListAccum<TUPLE_RECORD> @@tupleRecords;\n","\tmovies = {MOVIE.*};  \n","\tresult = SELECT tgt FROM movies:s-(:e1)-TERM:mid-(:e2)-MOVIE:tgt WHERE s != tgt \n","\t         ACCUM @@tupleRecords += TUPLE_RECORD (s.name, tgt.name);\n","\tPRINT @@tupleRecords;\n","}\n","```"]},{"cell_type":"markdown","metadata":{"id":"QvPS7TtM4Q_g","colab_type":"text"},"source":["## Installing Packages\n","The core packages that need to be installed are PyTorch, dgl, and pyTigerGraph. PyTorch and dgl are used for creating and training the GCN, while pyTigerGraph is used for connecting to the TigerGraph database. We also import networkx for converting the list of edges from TigerGraph into a graph dgl can work with."]},{"cell_type":"code","metadata":{"id":"WMXcHVwCxWtR","colab_type":"code","outputId":"3b3581bf-92d4-4480-f6ab-24bbd5eab714","executionInfo":{"status":"ok","timestamp":1579746297848,"user_tz":360,"elapsed":10392,"user":{"displayName":"Parker Erickson","photoUrl":"","userId":"09193906345779100690"}},"colab":{"base_uri":"https://localhost:8080/","height":399}},"source":["!pip install pyTigerGraph\n","!pip install torch torchvision\n","!pip install dgl\n","!pip install networkx"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pyTigerGraph in /usr/local/lib/python3.6/dist-packages (0.0.4.6)\n","Requirement already satisfied: validators in /usr/local/lib/python3.6/dist-packages (from pyTigerGraph) (0.14.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pyTigerGraph) (2.21.0)\n","Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from validators->pyTigerGraph) (4.4.1)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from validators->pyTigerGraph) (1.12.0)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pyTigerGraph) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pyTigerGraph) (1.24.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pyTigerGraph) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pyTigerGraph) (2019.11.28)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.4.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (6.2.2)\n","Requirement already satisfied: dgl in /usr/local/lib/python3.6/dist-packages (0.4.2)\n","Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.6/dist-packages (from dgl) (2.4)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (1.4.1)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (1.17.5)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.1->dgl) (4.4.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (2.4)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx) (4.4.1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HHNYpatu4iRl","colab_type":"text"},"source":["## Importing Packages\n","We now import the packages we just installed"]},{"cell_type":"code","metadata":{"id":"qZ2QOHOay_ri","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import pyTigerGraph as tg\n","import dgl\n","import networkx as nx\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nT07WDFn4ocg","colab_type":"text"},"source":["\n","##Configuration\n","\n","Here we define some variables, such as the number of epochs of training (usually only need 30 or less for a 2-layer GCN), the learning rate (0.01 seems to work well).\n"]},{"cell_type":"code","metadata":{"id":"cshhzHac28FQ","colab_type":"code","colab":{}},"source":["numEpochs = 25\n","learningRate = 0.01"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aKnPGSqkOio-","colab_type":"text"},"source":["## Creating the Graph Convolutional Neural Network\n","The block below defines some functions and classes for the GCN. The main ones to look at are the GCNLayer, which are the individual building blocks that the GCN class is made out of. The GCN class defines the structure of our neural network."]},{"cell_type":"code","metadata":{"id":"alp7UpjBzGWg","colab_type":"code","colab":{}},"source":["# Define the message and reduce function\n","# NOTE: We ignore the GCN's normalization constant c_ij for this tutorial.\n","def gcn_message(edges):\n","    # The argument is a batch of edges.\n","    # This computes a (batch of) message called 'msg' using the source node's feature 'h'.\n","    return {'msg' : edges.src['h']}\n","\n","def gcn_reduce(nodes):\n","    # The argument is a batch of nodes.\n","    # This computes the new 'h' features by summing received 'msg' in each node's mailbox.\n","    return {'h' : torch.sum(nodes.mailbox['msg'], dim=1)}\n","\n","# Define the GCNLayer module\n","class GCNLayer(nn.Module):\n","    def __init__(self, in_feats, out_feats):\n","        super(GCNLayer, self).__init__()\n","        self.linear = nn.Linear(in_feats, out_feats)\n","\n","    def forward(self, g, inputs):\n","        # g is the graph and the inputs is the input node features\n","        # first set the node features\n","        g.ndata['h'] = inputs\n","        # trigger message passing on all edges\n","        g.send(g.edges(), gcn_message)\n","        # trigger aggregation at all nodes\n","        g.recv(g.nodes(), gcn_reduce)\n","        # get the result node features\n","        h = g.ndata.pop('h')\n","        # perform linear transformation\n","        return self.linear(h)\n","\n","# Define a 2-layer GCN model\n","class GCN(nn.Module):\n","    def __init__(self, in_feats, hidden_size, num_classes):\n","        super(GCN, self).__init__()\n","        self.gcn1 = GCNLayer(in_feats, hidden_size)\n","        self.gcn2 = GCNLayer(hidden_size, num_classes)\n","\n","    def forward(self, g, inputs):\n","        h = self.gcn1(g, inputs)\n","        h = torch.relu(h)\n","        h = self.gcn2(g, h)\n","        return h"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fs42y-ml41FT","colab_type":"text"},"source":["\n","## Creating Database Connection and Creating Edge List\n","\n","This section instantiates a connection to the TigerGraph database and creates a list of tuples which consist of directed edges in the form of (from, to). This is done through two dictionaries that corresponds an article name to a unique numerical id that is needed to process the graph in the GCN.\n","\n","\n","#### **Assumption Alert:** We oversimplify the graph here. The query returns pairs of movies that share the same term (genre). In the real world, most people like a variety of genres and therefore their views are a little more nuanced than creating a graph where the edges are created if the movies share the same genre. Better link creation factors might be actors, directors, etc. but we don't have that in this dataset. Where TigerGraph comes in is the ease of data extraction, as there are no JOIN operations to create these links between movies.\n","* Note: It is possible to create a GCN that has multiple types of verticies, (known as a Relational Graph Convolutional Notebook) but it is more complex. A good way to get started is to simplify until you only have relations between the same type of thing.\n"]},{"cell_type":"code","metadata":{"id":"27w-wDGIzlxh","colab_type":"code","outputId":"d8ba2907-feef-46d2-89e7-38c766c20afc","executionInfo":{"status":"ok","timestamp":1579746303185,"user_tz":360,"elapsed":10199,"user":{"displayName":"Parker Erickson","photoUrl":"","userId":"09193906345779100690"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["graph = tg.TigerGraphConnection(\n","    ipAddress=\"https://graphml.i.tgcloud.io\", \n","    graphname=\"Recommender\", \n","    apiToken=\"bekr9ls24mlh4kbkd7g28stq8vpj67vi\") # Really not the best idea to have your API key out in the open, but for the sake of the demo, here it is\n","\n","movieToNum = {} # translation dictionary for movie name to number (for dgl)\n","numToMovie = {} # translation dictionary for number to movie name\n","i = 0\n","def createEdgeList(result): # returns tuple of number version of edge\n","    global i\n","    if result[\"src\"] in movieToNum:\n","        fromKey = movieToNum[result[\"src\"]]\n","    else:\n","        movieToNum[result[\"src\"]] = i\n","        numToMovie[i] = result[\"src\"]\n","        fromKey = i\n","        i+=1\n","    if result[\"dest\"] in movieToNum:\n","        toKey = movieToNum[result[\"dest\"]]\n","    else:\n","        movieToNum[result[\"dest\"]] = i\n","        numToMovie[i] = result[\"dest\"]\n","        toKey = i\n","        i+=1\n","    return (fromKey, toKey)\n","    \n","edges = [createEdgeList(thing) for thing in graph.runInstalledQuery(\"movieLinks\", {}, sizeLimit=128000000)[\"results\"][0][\"@@tupleRecords\"]] # creates list of edges\n","print(len(edges))\n","print(edges[:5])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1046378\n","[(0, 1), (2, 1), (3, 1), (4, 1), (5, 1)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"D_I2xumq49GH","colab_type":"text"},"source":["\n","## Initializing Graph\n","\n","This section converts the list of edges into a graph that DGL can process in the GCN. It also converts our liked and disliked movies to their corresponding numerical ids that we will use later on.\n"]},{"cell_type":"code","metadata":{"id":"_BNqh7fz0486","colab_type":"code","colab":{}},"source":["likedMovie = movieToNum[\"Sound of Music, The (1965)\"]\n","dislikedMovie = movieToNum[\"Mary Shelley's Frankenstein (1994)\"]\n","\n","g = nx.Graph()\n","g.add_edges_from(edges)\n","\n","\n","G = dgl.DGLGraph(g)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kcVoJBlO5B9C","colab_type":"text"},"source":["## Adding Features to Graph\n","We one-hot encode the features of the verticies in the graph. Feature assignment can be done a multitude of different ways, this is just the fastest and easiest, especially given the lack of attributal information in the dataset.\n","\n","If you had a graph of documents for example, you could run doc2vec on those documents to create a feature vector and create the feature matrix by concatenating those together.\n","\n","Another possiblity is that you have a graph of songs, artists, albums, etc. and you could use tempo, max volume, minimum volume, length, and other numerical descriptions of the song to create the feature matrix."]},{"cell_type":"code","metadata":{"id":"SFom5saG2iwI","colab_type":"code","outputId":"f1957845-0d8c-4fa6-a27b-19d575fee50b","executionInfo":{"status":"ok","timestamp":1579746308865,"user_tz":360,"elapsed":345,"user":{"displayName":"Parker Erickson","photoUrl":"","userId":"09193906345779100690"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["G.ndata[\"feat\"] = torch.eye(G.number_of_nodes())\n","\n","print(G.nodes[2].data['feat'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[0., 0., 1.,  ..., 0., 0., 0.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t5x-w6Nc5KBT","colab_type":"text"},"source":["\n","## Creating Neural Network and Labelling Relevant Verticies\n","\n","Here, we create the GCN. A two-layered GCN appears to work better than deeper networks, and this is further corroborated by the fact [this](https://arxiv.org/abs/1609.02907) paper only used a two-layered one. We also label the wanted and unwanted verticies and setup the optimizer.\n"]},{"cell_type":"code","metadata":{"id":"csr1za9Q2lFt","colab_type":"code","colab":{}},"source":["net = GCN(G.number_of_nodes(), 15, 2) #Two layer GCN\n","inputs = G.ndata[\"feat\"]\n","labeled_nodes = torch.tensor([likedMovie, dislikedMovie])  # only the liked movie and disliked movie are labelled\n","labels = torch.tensor([0, 1])  # their labels are different\n","optimizer = torch.optim.Adam(net.parameters(), lr=learningRate)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bSu33Ee2YKdI","colab_type":"text"},"source":["## Training Loop\n","Below is the training loop that actually trains the GCN. Unlike many traditional deep learning architectures, GCNs don't always need that much training or as large of data sets due to their exploitation of the *structure* of the data, as opposed to only the features of the data.\n","* Note: due to the randomized initial values of the weights in the neural network, sometimes models don't work very well, or their loss gets stuck at a relatively large number. If that happens, just stop and restart the training process and hope for better luck!"]},{"cell_type":"code","metadata":{"id":"O8NIp6qh2z32","colab_type":"code","outputId":"f3cbf9de-700d-4823-ad2c-7f9f21c8c445","executionInfo":{"status":"ok","timestamp":1579722044387,"user_tz":360,"elapsed":524025,"user":{"displayName":"Parker Erickson","photoUrl":"","userId":"09193906345779100690"}},"colab":{"base_uri":"https://localhost:8080/","height":471}},"source":["all_logits = []\n","for epoch in range(numEpochs):\n","    logits = net(G, inputs)\n","    # we save the logits for visualization later\n","    all_logits.append(logits.detach())\n","    logp = F.log_softmax(logits, 1)\n","    # we only compute loss for labeled nodes\n","    loss = F.nll_loss(logp[labeled_nodes], labels)\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    print('Epoch %d | Loss: %6.3e' % (epoch, loss.item()))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 0 | Loss: 1.579e+00\n","Epoch 1 | Loss: 2.732e+03\n","Epoch 2 | Loss: 5.575e+02\n","Epoch 3 | Loss: 4.361e-01\n","Epoch 4 | Loss: 6.102e-01\n","Epoch 5 | Loss: 6.769e-01\n","Epoch 6 | Loss: 6.878e-01\n","Epoch 7 | Loss: 6.941e-01\n","Epoch 8 | Loss: 6.944e-01\n","Epoch 9 | Loss: 6.945e-01\n","Epoch 10 | Loss: 6.946e-01\n","Epoch 11 | Loss: 6.947e-01\n","Epoch 12 | Loss: 6.947e-01\n","Epoch 13 | Loss: 6.948e-01\n","Epoch 14 | Loss: 6.948e-01\n","Epoch 15 | Loss: 6.948e-01\n","Epoch 16 | Loss: 6.948e-01\n","Epoch 17 | Loss: 6.948e-01\n","Epoch 18 | Loss: 6.948e-01\n","Epoch 19 | Loss: 6.948e-01\n","Epoch 20 | Loss: 6.947e-01\n","Epoch 21 | Loss: 6.947e-01\n","Epoch 22 | Loss: 6.947e-01\n","Epoch 23 | Loss: 6.946e-01\n","Epoch 24 | Loss: 6.946e-01\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PnINDzkF5lvg","colab_type":"text"},"source":["\n","## Output Predictions\n","\n","This section translates the output of the last result of training and outputs the top 10 results given the liked movie.\n"]},{"cell_type":"code","metadata":{"id":"NMifrddk3Ew_","colab_type":"code","outputId":"9179ae98-4966-482c-fc48-bab55bbcb767","executionInfo":{"status":"ok","timestamp":1579722044391,"user_tz":360,"elapsed":524024,"user":{"displayName":"Parker Erickson","photoUrl":"","userId":"09193906345779100690"}},"colab":{"base_uri":"https://localhost:8080/","height":744}},"source":["predictions = list(all_logits[numEpochs-1])\n","\n","predictionsWithIndex = []\n","a = 0\n","for movie in predictions:\n","    predictionsWithIndex.append([a, movie[0]])\n","    a+=1\n","\n","predictionsWithIndex.sort(key=lambda x: x[1], reverse=True)\n","\n","topResults = predictionsWithIndex[:10]\n","\n","\n","for movie in topResults:\n","    print(\"movie Id: \"+str(movie[0]))\n","    print(\"movie Name: \"+str(numToMovie[movie[0]]))\n","    print(\"movie Score: \"+str(movie[1]))\n","    print(\"\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["movie Id: 0\n","movie Name: Relic, The (1997)\n","movie Score: tensor(-0.1071)\n","\n","movie Id: 1\n","movie Name: Nosferatu a Venezia (1986)\n","movie Score: tensor(-0.1071)\n","\n","movie Id: 2\n","movie Name: Alien: Resurrection (1997)\n","movie Score: tensor(-0.1071)\n","\n","movie Id: 3\n","movie Name: Amityville Horror, The (1979)\n","movie Score: tensor(-0.1071)\n","\n","movie Id: 4\n","movie Name: Mary Shelley's Frankenstein (1994)\n","movie Score: tensor(-0.1071)\n","\n","movie Id: 5\n","movie Name: Alien 3 (1992)\n","movie Score: tensor(-0.1071)\n","\n","movie Id: 6\n","movie Name: Body Snatcher, The (1945)\n","movie Score: tensor(-0.1071)\n","\n","movie Id: 7\n","movie Name: Cat People (1982)\n","movie Score: tensor(-0.1071)\n","\n","movie Id: 8\n","movie Name: Amityville Curse, The (1990)\n","movie Score: tensor(-0.1071)\n","\n","movie Id: 9\n","movie Name: Blood Beach (1981)\n","movie Score: tensor(-0.1071)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"svNtInV3FLQx","colab_type":"text"},"source":["## Test Predictions with User Data"]},{"cell_type":"code","metadata":{"id":"sy2JYHl6Fj34","colab_type":"code","outputId":"20f92b4b-affb-4b80-c09b-d931e0c51018","executionInfo":{"status":"error","timestamp":1579748021305,"user_tz":360,"elapsed":709,"user":{"displayName":"Parker Erickson","photoUrl":"","userId":"09193906345779100690"}},"colab":{"base_uri":"https://localhost:8080/","height":263}},"source":["from heapq import nlargest, nsmallest\n","ratings = graph.runInstalledQuery(\"userRatings\", {\"user\":\"13\"})[\"results\"][0][\"S1\"]\n","top3Movies = [thing[\"attributes\"][\"movieTitle\"] for thing in nlargest(3, ratings, key=lambda item: item[\"attributes\"][\"userRating\"])] # getting the 3 highest rated movies by the user\n","bottom3Movies = [thing[\"attributes\"][\"movieTitle\"] for thing in nsmallest(3, ratings, key=lambda item: item[\"attributes\"][\"userRating\"])] # getting the 3 lowest rated movies by the user\n","unclassifiedMovies = list(set(ratings) - set([thing for thing in nlargest(3, ratings, key=lambda item: item[\"attributes\"][\"userRating\"])]) - set([thing for thing in nsmallest(3, ratings, key=lambda item: item[\"attributes\"][\"userRating\"])]))\n","\n","def filterNegative(thing):\n","  if thing[\"attributes\"][\"userRating\"] < 0:\n","    return thing\n","\n","negativeRating = [filterNegative(thing) for thing in unclassifiedMovies]\n","positiveRating = list(set(unclassifiedMovies)-set(negativeRating))\n","print(len(unclassifiedMovies))\n","print(len(negativeRating))\n","print(len(positiveRating))\n","print(top3Movies)\n","print(bottom3Movies)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-c7b60e8933ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtop3Movies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mthing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attributes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"movieTitle\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthing\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlargest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attributes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"userRating\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# getting the 3 highest rated movies by the user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbottom3Movies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mthing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attributes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"movieTitle\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthing\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnsmallest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attributes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"userRating\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# getting the 3 lowest rated movies by the user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0munclassifiedMovies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthing\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthing\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlargest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attributes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"userRating\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthing\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthing\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnsmallest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attributes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"userRating\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfilterNegative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"]}]},{"cell_type":"code","metadata":{"id":"OXBG-rrxFSgC","colab_type":"code","colab":{}},"source":["net = GCN(G.number_of_nodes(), 20, 2) #Two layer GCN\n","inputs = G.ndata[\"feat\"]\n","labeled_nodes = torch.tensor([movieToNum[top3Movies[0]], movieToNum[top3Movies[1]], movieToNum[top3Movies[2]], \n","                              movieToNum[bottom3Movies[0]], movieToNum[bottom3Movies[1]], movieToNum[bottom3Movies[2]]])  # only the liked movies and the disliked movies are labelled\n","labels = torch.tensor([0, 0, 0, 1, 1, 1])  # their labels are different\n","optimizer = torch.optim.Adam(net.parameters(), lr=learningRate)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AgkiuuFfOEE-","colab_type":"code","outputId":"97674529-66b0-4d5c-a24e-d2606f00b1fe","executionInfo":{"status":"ok","timestamp":1579747505202,"user_tz":360,"elapsed":532107,"user":{"displayName":"Parker Erickson","photoUrl":"","userId":"09193906345779100690"}},"colab":{"base_uri":"https://localhost:8080/","height":471}},"source":["all_logits = []\n","for epoch in range(numEpochs):\n","    logits = net(G, inputs)\n","    # we save the logits for visualization later\n","    all_logits.append(logits.detach())\n","    logp = F.log_softmax(logits, 1)\n","    # we only compute loss for labeled nodes\n","    loss = F.nll_loss(logp[labeled_nodes], labels)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    print('Epoch %d | Loss: %6.3e' % (epoch, loss.item()))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 0 | Loss: 7.528e-01\n","Epoch 1 | Loss: 4.858e+02\n","Epoch 2 | Loss: 2.551e+02\n","Epoch 3 | Loss: 4.904e+01\n","Epoch 4 | Loss: 0.000e+00\n","Epoch 5 | Loss: 0.000e+00\n","Epoch 6 | Loss: 3.852e+01\n","Epoch 7 | Loss: 0.000e+00\n","Epoch 8 | Loss: 0.000e+00\n","Epoch 9 | Loss: 0.000e+00\n","Epoch 10 | Loss: 0.000e+00\n","Epoch 11 | Loss: 0.000e+00\n","Epoch 12 | Loss: 0.000e+00\n","Epoch 13 | Loss: 0.000e+00\n","Epoch 14 | Loss: 0.000e+00\n","Epoch 15 | Loss: 0.000e+00\n","Epoch 16 | Loss: 0.000e+00\n","Epoch 17 | Loss: 0.000e+00\n","Epoch 18 | Loss: 0.000e+00\n","Epoch 19 | Loss: 0.000e+00\n","Epoch 20 | Loss: 0.000e+00\n","Epoch 21 | Loss: 0.000e+00\n","Epoch 22 | Loss: 0.000e+00\n","Epoch 23 | Loss: 0.000e+00\n","Epoch 24 | Loss: 0.000e+00\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VRbhbVbhOGX1","colab_type":"code","outputId":"ac0c29a2-bb87-4acf-ff17-a877a6f35f7a","executionInfo":{"status":"ok","timestamp":1579747626743,"user_tz":360,"elapsed":309,"user":{"displayName":"Parker Erickson","photoUrl":"","userId":"09193906345779100690"}},"colab":{"base_uri":"https://localhost:8080/","height":782}},"source":["predictions = list(all_logits[numEpochs-1])\n","\n","predictionsWithIndex = []\n","a = 0\n","for movie in predictions:\n","    predictionsWithIndex.append([a, movie[0]])\n","    a+=1\n","\n","predictionsWithIndex.sort(key=lambda x: x[1], reverse=True)\n","\n","topResults = predictionsWithIndex[:10]\n","\n","\n","for movie in topResults:\n","    print(\"movie Id: \"+str(movie[0]))\n","    print(\"movie Name: \"+str(numToMovie[movie[0]]))\n","    print(\"movie Score: \"+str(movie[1]))\n","    print(\"\")\n","\n","actualTop10 = [thing[\"attributes\"][\"movieTitle\"] for thing in nlargest(10, ratings, key=lambda item: item[\"attributes\"][\"userRating\"])]\n","print(actualTop10)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["movie Id: 211\n","movie Name: Pollyanna (1960)\n","movie Score: tensor(720.8066)\n","\n","movie Id: 270\n","movie Name: Babe (1995)\n","movie Score: tensor(720.8066)\n","\n","movie Id: 158\n","movie Name: This Is Spinal Tap (1984)\n","movie Score: tensor(677.1639)\n","\n","movie Id: 530\n","movie Name: Stand by Me (1986)\n","movie Score: tensor(655.3315)\n","\n","movie Id: 971\n","movie Name: Ma vie en rose (My Life in Pink) (1997)\n","movie Score: tensor(653.3358)\n","\n","movie Id: 983\n","movie Name: Sirens (1994)\n","movie Score: tensor(653.3358)\n","\n","movie Id: 996\n","movie Name: Reluctant Debutante, The (1958)\n","movie Score: tensor(653.3358)\n","\n","movie Id: 1000\n","movie Name: Last Summer in the Hamptons (1995)\n","movie Score: tensor(653.3358)\n","\n","movie Id: 1002\n","movie Name: Beat the Devil (1954)\n","movie Score: tensor(653.3358)\n","\n","movie Id: 1005\n","movie Name: Stefano Quantestorie (1993)\n","movie Score: tensor(653.3358)\n","\n","['Steel (1997)', 'Phantom, The (1996)', 'Baby-Sitters Club, The (1995)', 'Shadow, The (1994)', 'Timecop (1994)', 'Mulholland Falls (1996)', 'Life Less Ordinary, A (1997)', 'Wyatt Earp (1994)', 'Boys Life (1995)', 'Shaggy Dog, The (1959)']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JnzAPzqXaV34","colab_type":"text"},"source":["## Credits:\n","<p><img alt=\"Picture of Parker Erickson\" height=\"150px\" src=\"https://avatars1.githubusercontent.com/u/9616171?s=460&v=4\" align=\"right\" hspace=\"20px\" vspace=\"20px\"></p>\n","\n","Demo/tutorial written by Parker Erickson, a student at the University of Minnesota pursuing a B.S. in Computer Science. His interests include graph databases, machine learning, travelling, playing the saxophone, and watching Minnesota Twins baseball. Feel free to reach out! Find him on:\n","\n","* LinkedIn: [https://www.linkedin.com/in/parker-erickson/](https://www.linkedin.com/in/parker-erickson/)\n","* GitHub: [https://github.com/parkererickson](https://github.com/parkererickson)\n","* Medium: [https://medium.com/@parker.erickson](https://medium.com/@parker.erickson)\n","* Email: parker.erickson30@gmail.com\n","----\n","GCN Resources:\n","* DGL Documentation: [https://docs.dgl.ai/](https://docs.dgl.ai/)\n","* GCN paper by Kipf and Welling [https://arxiv.org/abs/1609.02907](https://arxiv.org/abs/1609.02907)\n","* R-GCN paper: [https://arxiv.org/abs/1703.06103](https://arxiv.org/abs/1703.06103)\n","---- \n","Notebook adapted from: [https://docs.dgl.ai/en/latest/tutorials/basics/1_first.html](https://docs.dgl.ai/en/latest/tutorials/basics/1_first.html)"]}]}