{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language processing - Basic Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites for this course\n",
    "1. Basic Python Programming \n",
    "2. Basic Python Libraries\n",
    "3. Webscraping basics using beautiful soup<br>\n",
    "\n",
    "Incase you do not have the necessary knowledge of these topics kindly check out our Refactored.ai course on the same:\n",
    "1. [Python for Data Scientists](https://refactored.ai/course/python-for-data-scientists/)\n",
    "2. [Web Scraping using Python](https://refactored.ai/learn/web-scraping/b84d97e62c9a4f96bfac48be769e62e5/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization, Stemming & Lemmatization, Part-of-speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization\n",
    "Every NLP exrercise requires us to break up words or tokenize them.\n",
    "So before we can effectively count the words or organize in a running text we need to figure out whether we want to normalize those words are not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What normalization does is that it breaks down the tokens that are in text or segments the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this can be divided into 3 main categories:\n",
    "1. Segmentation/tokenizing words in running text\n",
    "2. Normalizing word formats \n",
    "3. Segmenting sentences in running text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we do Text Normalization?\n",
    "Let us look at an exercise:\n",
    "    \n",
    "__How many words?__ <br>\n",
    "I do uh main- mainly business data processing\n",
    "\n",
    "__Answer__<br>\n",
    "The answer could range from 6-8 words.<br>\n",
    "If one does not count \"uh main-\" as proper words it comes down to 6 words<br>\n",
    "If one counts like a computer it comes down to 8 words<br>\n",
    "\n",
    "Hence Text normalization is very important based on your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at another exercise:\n",
    "\n",
    "__Count the number of unique words:__<br>\n",
    "Susan's cat in the hat is different from other cats!\n",
    "\n",
    "__Answer__<br>\n",
    "The answer could range from 9-10 depending on whether we take cat or cats as a single word.<br>\n",
    "This again comes to the fact that if we want to find topics in a paragraph and want to highlight cat we should consider cat and cats as the same and will need to normalize them\n",
    "\n",
    "* Lemma: Same stem, part of speech, rough word sense<br>\n",
    " - cat and cats = same lemma\n",
    "* Wordform: the full inflected surface form\n",
    " - cat and cats = different wordforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Type:__ an element of the vocabulary. Example would be unique words\n",
    "* __Token:__ an instance of that type in running text. Example would be every single word in a text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to download only the popular NLTK library.<br>\n",
    "We will primarily use the NLTK library from google which is Google's natural language toolkit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\manas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\manas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\manas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk as nltk\n",
    "import nltk.corpus  \n",
    "from nltk.text import Text\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Flu', 'season', 'hitting', 'earlier', ',', 'with', 'dozens', 'more', 'outbreaks', '—', 'and', 'more', 'severe', 'symptoms']\n"
     ]
    }
   ],
   "source": [
    "#Tokenizing using nltk function called word_tokenize\n",
    "text = \"Flu season hitting earlier, with dozens more outbreaks — and more severe symptoms\"\n",
    "tokens = nltk.tokenize.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Write a simple code to tokenize the following text:\n",
    "\n",
    "'What a pity that youth must be wasted on the young.' - George Bernard Shaw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'What\", 'a', 'pity', 'that', 'youth', 'must', 'be', 'wasted', 'on', 'the', 'young', '.', \"'\", '-', 'George', 'Bernard', 'Shaw']\n"
     ]
    }
   ],
   "source": [
    "text = \"'What a pity that youth must be wasted on the young.' - George Bernard Shaw\"\n",
    "tokens = nltk.tokenize.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get most frequent words in a book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 7773 samples and 79643 outcomes>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(',', 5702),\n",
       " ('the', 3338),\n",
       " ('and', 3215),\n",
       " ('.', 3081),\n",
       " ('to', 1748),\n",
       " ('a', 1621),\n",
       " ('of', 1425),\n",
       " ('I', 1208),\n",
       " ('it', 1159),\n",
       " ('in', 931)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#open book called 3boat10\n",
    "book = '3boat10.txt'\n",
    "f = open(book)\n",
    "bk_3boat = f.read()\n",
    "\n",
    "\n",
    "#Tokenize the words in the book\n",
    "words = nltk.tokenize.word_tokenize(bk_3boat)\n",
    "\n",
    "#use FreqDist to retireve token frequency\n",
    "fdist = nltk.FreqDist(words)\n",
    "\n",
    "print(fdist)\n",
    "\n",
    "#print most common words\n",
    "fdist.most_common(10)\n",
    "#fdist.items() - will give all words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The previous output gives us no idea what the book is about beacuse it captures the most common tokens present in the book which are primarily stop words(eg. the and to). Hence we will require to clean our data from these stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get most frequent clean words in a book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 6241 samples and 29843 outcomes>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('said', 378),\n",
       " ('would', 362),\n",
       " ('harris', 316),\n",
       " ('george', 308),\n",
       " ('one', 246),\n",
       " ('us', 228),\n",
       " ('boat', 186),\n",
       " ('get', 179),\n",
       " ('could', 175),\n",
       " ('got', 163)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "\n",
    "#default_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "words = nltk.tokenize.word_tokenize(bk_3boat)\n",
    "\n",
    "#stopwords = stopwords.words('english')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Remove single-character tokens (mostly punctuation)\n",
    "words = [word for word in words if len(word) > 1]\n",
    "\n",
    "# Remove numbers\n",
    "#words = [word for word in words if not word.isnumeric()]\n",
    "\n",
    "# Remove punctuation\n",
    "words = [word for word in words if word.isalpha()]\n",
    "\n",
    "# Lowercase all words (default_stopwords are lowercase too)\n",
    "words_lc = [word.lower() for word in words]\n",
    "\n",
    "# Remove stopwords\n",
    "words_lc = [word for word in words_lc if word not in stopwords]\n",
    "\n",
    "# Remove stopwords\n",
    "# words = [word for word in words if word not in stopwords]\n",
    "\n",
    "\n",
    "fdist = nltk.FreqDist(words_lc)\n",
    "\n",
    "print(fdist)\n",
    "\n",
    "#fdist.items() - will give all words\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have to instantiate a Text object first, and then call it on that object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'D:/Colaberry/NLP Course/'\n",
    "textList = Text(nltk.corpus.gutenberg.words(directory+book))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concordance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A concordance view shows us every occurrence of a given word, together with some context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It will helps us understand in which context, the water is frequently used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 88 matches:\n",
      " thin , not the captain ) and soda - water ; but , towards Saturday , he got up\n",
      "night ,\" and , lulled by the lapping water and the rustling trees , we fall asl\n",
      " , and there is a good two inches of water in the boat , and all the things are\n",
      "t the boat , and who has spilled the water down his sleeve , and has been cursi\n",
      " good , plain merchandise will stand water . You will have time to think as wel\n",
      "hen they are going anywhere near the water , but that they don ' t bathe much w\n",
      " , shivering , through six inches of water . And when I do get to the sea , it \n",
      " swimming for my life in two feet of water . I hop back and dress , and crawl h\n",
      "of Harris ' s , which you mixed with water and called lemonade , plenty of tea \n",
      "orted . \" Now we shan ' t get on the water till after twelve . I wonder you tak\n",
      "on , and prognosticate drought , and water famine , and sunstroke , and simooms\n",
      "the lower part of the town was under water , owing to the river having overflow\n",
      "ident he was the 9 . 32 for Virginia Water , or the 10 a . m . express for the \n",
      "ngston , where they came down to the water ' s edge , looked quite picturesque \n",
      " cloaked gallants swaggered down the water - steps to cry : \" What Ferry , ho !\n",
      "metimes , when you could not see any water at all , but only a brilliant tangle\n",
      "easant landscape , and the sparkling water , it is one of the gayest sights I k\n",
      "t was my misfortune once to go for a water picnic with two ladies of this kind \n",
      "anywhere near real earth , air , and water . The first thing was that they thou\n",
      "ing , and it appeared that a drop of water ruined those costumes . The mark nev\n",
      "m , and I picked out a smooth bit of water to drop them into again each time . \n",
      "ld not help an occasional flicker of water from going over those dresses . The \n",
      "e . When he spread more than pint of water over one of those dresses , he would\n",
      ", and sloush the things about in the water .\" The elder sister said that she wa\n",
      "n the hamper , and a gallon - jar of water in the nose of the boat , and that t\n"
     ]
    }
   ],
   "source": [
    "textList.concordance(\"water\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Exercise\n",
    "\n",
    "Write a simple code to find the concordance of the following text:\n",
    "\n",
    "dog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 31 matches:\n",
      "MEN IN A BOAT ( TO SAY NOTHING OF THE DOG ). Three Men in a Boat by Jerome K . \n",
      "ed up at me , and think : \" Oh , that dog will never live . He will be snatched\n",
      "t door but one for having a ferocious dog at large , that had kept him pinned u\n",
      "e and someone to love you , a cat , a dog , and a pipe or two , enough to eat a\n",
      "ed him . I didn ' t encourage him . A dog like that don ' t want any encouragem\n",
      " with eggs and bacon , irritating the dog , or flirting with the slavey , inste\n",
      "dly . He would take bronchitis in the dog - days , and have hay - fever at Chri\n",
      "by the lady of the house ? That china dog that ornaments the bedroom of my furn\n",
      "my furnished lodgings . It is a white dog . Its eyes blue . Its nose is a delic\n",
      "me it is more than probable that that dog will be dug up from somewhere or othe\n",
      "s age , do not see the beauty of that dog . We are too familiar with it . It is\n",
      "o our eyes . So it is with that china dog . In 2288 people will gush over it . \n",
      " one another , and we beamed upon the dog , too . We loved each other , we love\n",
      "INE TO DRINK THE RIVER . - A PEACEFUL DOG . - STRANGE DISAPPEARANCE OF HARRIS A\n",
      "life , with care . I do not blame the dog ( contenting myself , as a rule , wit\n",
      "but mangy about the middle ; a bull - dog , a few Lowther Arcade sort of animal\n",
      "chained up there , between the bull - dog and the poodle . He sat and looked ab\n",
      "d dignified . He looked at the bull - dog , sleeping dreamlessly on his right .\n",
      "his own place , and caught the bull - dog by the ear , and tried to throw him a\n",
      "ed to throw him away ; and the bull - dog , a curiously impartial animal , went\n",
      "d , and snatched up that sweet little dog of hers ( he had laid the tyke up for\n",
      "have chilled the heart of the boldest dog . He stopped abruptly , and looked ba\n",
      "' s boy , with basket . Long - haired dog . Cheesemonger ' s boy , with basket \n",
      "owards us on the sluggish current , a dog . It was one of the quietest and peac\n",
      "dogs I have ever seen . I never met a dog who seemed more contented - more easy\n"
     ]
    }
   ],
   "source": [
    "textList.concordance(\"dog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using \"similar\" helps us discover what other words appear in a similar range of contexts\n",
    "\n",
    "Almost the same functionality as the concordance, so it determines the word similarity based on the world's at the frequency of appearing to the right and to the left of the world of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "river boat thing time room bank morning night man things sea lock it\n",
      "other way place house subject them matter\n"
     ]
    }
   ],
   "source": [
    "textList.similar(\"water\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bit man long morning change dream body widow party trout boat harris\n",
      "hundred rest week river mean he out is\n"
     ]
    }
   ],
   "source": [
    "textList.similar(\"dog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional information helps determine the location of a word in the text: how many words from the beginning it appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline \n",
    "\n",
    "textList.dispersion_plot([\"boat\", \"dog\", \"river\", \"lunch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By default tokenization includes all surrounting punctuation charachters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81185"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(textList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can invoke RegexpTokenizer to eliminate punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68364"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This will match any word characters until it reaches a non-word character, like a space\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(bk_3boat)\n",
    "\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring lexical diversity: dividing unique words by overall words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use lexical diversity as a measure to help understand type of text usage. Example would be identifying plagiarism between texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.222146948327893"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(textList)) / len(textList)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "* Need to \"normalize\" terms\n",
    " * Information Reterival: indexed text & query terms must have same form.<br>\n",
    " We want to match __U.S.A.__ and __USA__\n",
    "* We implicitly define equivalence classes of terms\n",
    " * e.eg., deleting periods in a term\n",
    "* Alternative: asymmetric explansion:\n",
    " * Enter: __window__ Search: __window, windows__\n",
    " * Enter __windows__ Search: __Windows, windows, window__\n",
    " * Enter: __Windows__ Search: __Windows__\n",
    " \n",
    "The symmetric expansion is going to give us a huge explosion of data.<br>\n",
    "Versus bringing words to the most common word forum will give us the fastest possible results because we'll be simplifying the words, but there is a little bit of a trade off.\n",
    "\n",
    "Example: __windows__ and __Windows__ might mean, __window__(present in a house or shelter) and __Windows__ which is the operating system.<br>\n",
    "So depending depending on the use case, the semantic expansion can be more powerful, but it requires a lot more data storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case folding\n",
    "* Applications like Information Retrival: reduce all letters to lower case\n",
    " * Since users end to use lower case\n",
    " * Possible exception: upper case in mid-sentence?\n",
    "   - e.g. __General Motors__\n",
    "   - __Fed__ vs __fed__\n",
    "   - __SAIL__ vs __sail__\n",
    "* For sentiment analysis, Information Retrival\n",
    " * Case us helpful __US__ versus __us__ is important\n",
    " \n",
    "The other thing is very critical about case folding his name name recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text normalization with stemming and lemmatization\n",
    "\n",
    "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance:\n",
    "\n",
    "    am, are, is =>  be\n",
    "    dog, dogs, dog's, dogs' => dog\n",
    "\n",
    "The result of this mapping of text will be something like:\n",
    "\n",
    "    the girl's dogs are different breeds => the girl dog be differ breed \n",
    "    \n",
    "__Lemmatization__: have to find correct dictionary headword form\n",
    "\n",
    "### Morphemes:\n",
    "* The small meaningful units that make up words\n",
    "* Stems: The core meaning-bearing units\n",
    "* Affixes: Bits and pieces that adhere to stems\n",
    " * Often with grammatical functions\n",
    " \n",
    "The idea is that every word contains a stem and stem is the core mean bearing unit in each word. So if we chop the suffixes enough excess of each word we can get back to the stem.\n",
    "\n",
    "### Stemming\n",
    "* Reduce terms to their stems in information retrieval\n",
    "* Stemming is crude chopping of affixes\n",
    " * language dependent\n",
    " * e.g., __automate(s), autmatic,automation__ all reduced to __automat__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two major words stemming algorithms\n",
    "1. Porter's algorithm\n",
    "2. Lancaster algorithm\n",
    "\n",
    "This algorithm has 2 steps to it:\n",
    "\n",
    "Step 1a<br>\n",
    "sses $\\to$ ss   caresses$\\to$caress<br>\n",
    "les $\\to$ i     ponies$\\to$poni<br>\n",
    "ss $\\to$ ss     caress$\\to$caress<br>\n",
    "s $\\to$ (blank) cats$\\to$cat<br>\n",
    "\n",
    "Step 1b<br>\n",
    "Verbs<br>\n",
    "(*v*)ing $\\to$ (blank) walking $\\to$ walk<br>\n",
    "(*v*)ed $\\to$ (blank) plastered $\\to$ plaster\n",
    "\n",
    "Step 2 (for long stems)<br>\n",
    "ational $\\to$ ate relational $\\to$ relate<br>\n",
    "izer $\\to$ ize digitizer $\\to$ digitize<br>\n",
    "ator $\\to$ ate operator $\\to$ operate<br>\n",
    "\n",
    "Step 3(for longer stems)<br>\n",
    "al $\\to$ (blank) revival $\\to$ reviv<br>\n",
    "able $\\to$ (blank) adjustable $\\to$ adjust<br>\n",
    "ate $\\to$ (blank) activate $\\to$ activ<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting lists to strings to simplify displaying / visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_l = (words_lc[0:50])\n",
    "words_s = ', '.join(words_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['three', 'men', 'boat', 'say', 'nothing', 'dog', 'three', 'men', 'boat', 'jerome', 'jerome', 'chapter', 'three', 'invalids', 'sufferings', 'george', 'harris', 'victim', 'one', 'hundred', 'seven', 'fatal', 'maladies', 'useful', 'prescriptions', 'cure', 'liver', 'complaint', 'children', 'agree', 'overworked', 'need', 'rest', 'week', 'rolling', 'deep', 'george', 'suggests', 'river', 'montmorency', 'lodges', 'objection', 'original', 'motion', 'carried', 'majority', 'three', 'one', 'four', 'us']\n"
     ]
    }
   ],
   "source": [
    "print (words_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['three', 'men', 'boat', 'say', 'noth', 'dog', 'three', 'men', 'boat', 'jerom', 'jerom', 'chapter', 'three', 'invalid', 'suffer', 'georg', 'harri', 'victim', 'one', 'hundr', 'seven', 'fatal', 'maladi', 'use', 'prescript', 'cure', 'liver', 'complaint', 'children', 'agre', 'overwork', 'need', 'rest', 'week', 'roll', 'deep', 'georg', 'suggest', 'river', 'montmor', 'lodg', 'object', 'origin', 'motion', 'carri', 'major', 'three', 'one', 'four', 'us']\n"
     ]
    }
   ],
   "source": [
    "#Porters stemming\n",
    "\n",
    "print([porter.stem(t) for t in words_lc[0:50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['three', 'men', 'boat', 'say', 'noth', 'dog', 'three', 'men', 'boat', 'jerom', 'jerom', 'chapt', 'three', 'invalid', 'suff', 'georg', 'har', 'victim', 'on', 'hundr', 'sev', 'fat', 'malady', 'us', 'prescrib', 'cur', 'liv', 'complaint', 'childr', 'agr', 'overwork', 'nee', 'rest', 'week', 'rol', 'deep', 'georg', 'suggest', 'riv', 'montm', 'lodg', 'object', 'origin', 'mot', 'carry', 'maj', 'three', 'on', 'four', 'us']\n"
     ]
    }
   ],
   "source": [
    "#lancaster stemming\n",
    "\n",
    "print([lancaster.stem(t) for t in words_lc[0:50]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As stemming removed a lot of meaning from many words eg. george became georg, we will try a new method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "#### Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
    "\n",
    "The WordNet lemmatizer only removes affixes if the resulting word is in its dictionary. The dictionary checking makes lemmatizers significantly slower than stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['three', 'men', 'boat', 'say', 'nothing', 'dog', 'three', 'men', 'boat', 'jerome', 'jerome', 'chapter', 'three', 'invalids', 'sufferings', 'george', 'harris', 'victim', 'one', 'hundred', 'seven', 'fatal', 'maladies', 'useful', 'prescriptions', 'cure', 'liver', 'complaint', 'children', 'agree', 'overworked', 'need', 'rest', 'week', 'rolling', 'deep', 'george', 'suggests', 'river', 'montmorency', 'lodges', 'objection', 'original', 'motion', 'carried', 'majority', 'three', 'one', 'four', 'us']\n"
     ]
    }
   ],
   "source": [
    "print (words_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['three', 'men', 'boat', 'say', 'nothing', 'dog', 'three', 'men', 'boat', 'jerome', 'jerome', 'chapter', 'three', 'invalid', 'suffering', 'george', 'harris', 'victim', 'one', 'hundred', 'seven', 'fatal', 'malady', 'useful', 'prescription', 'cure', 'liver', 'complaint', 'child', 'agree', 'overworked', 'need', 'rest', 'week', 'rolling', 'deep', 'george', 'suggests', 'river', 'montmorency', 'lodge', 'objection', 'original', 'motion', 'carried', 'majority', 'three', 'one', 'four', 'u']\n"
     ]
    }
   ],
   "source": [
    "print([wnl.lemmatize(t) for t in words_lc[0:50]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As WordNetLemmatizer compares each word against a dictionary it is relatively slower than the simple porter and lancaster stemming hence its based on use case which one we want to use eg. for 10,000 records we can use Wordnet but for 500 million tweets we should use simple lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of speech tagging is important in NLP because we can use them in the following applications:\n",
    "* Text-to-Speech (how do we pronounce \"lead\"?)\n",
    "* Can write regexps like (Det)Adj* N+ over the output phrases, etc\n",
    "* As inout to or to speed up a full parser\n",
    "* If you know the tag, we can back off to it in other tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tag list:<br>\n",
    "<br>\n",
    "CC coordinating conjunction<br>\n",
    "CD cardinal digit<br>\n",
    "DT determiner<br>\n",
    "EX existential there (like: \"there is\" ... think of it like \"there exists\")<br>\n",
    "FW foreign word<br>\n",
    "IN preposition/subordinating conjunction<br>\n",
    "JJ adjective 'big'<br>\n",
    "JJR adjective, comparative 'bigger'<br>\n",
    "JJS adjective, superlative 'biggest'<br>\n",
    "LS list marker 1)<br>\n",
    "MD modal could, will<br>\n",
    "NN noun, singular 'desk'<br>\n",
    "NNS noun plural 'desks'<br>\n",
    "NNP proper noun, singular 'Harrison'<br>\n",
    "NNPS proper noun, plural 'Americans'<br>\n",
    "PDT predeterminer 'all the kids'<br>\n",
    "POS possessive ending parent's<br>\n",
    "PRP personal pronoun I, he, she<br>\n",
    "PRP possessive pronoun my, his, hers<br>\n",
    "RB adverb very, silently,<br>\n",
    "RBR adverb, comparative better<br>\n",
    "RBS adverb, superlative best<br>\n",
    "RP particle give up<br>\n",
    "TO to go 'to' the store.<br>\n",
    "UH interjection errrrrrrrm<br>\n",
    "VB verb, base form take<br>\n",
    "VBD verb, past tense took<br>\n",
    "VBG verb, gerund/present participle taking<br>\n",
    "VBN verb, past participle taken<br>\n",
    "VBP verb, sing. present, non-3d take<br>\n",
    "VBZ verb, 3rd person sing. present takes<br>\n",
    "WDT wh-determiner which<br>\n",
    "WP wh-pronoun who, what<br>\n",
    "WP possessive wh-pronoun whose<br>\n",
    "WRB wh-abverb where, when<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can use help function to get explanations of endividual tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "uc1 = \"The University of Chicago is a private research university in Chicago, Illinois\"\n",
    "uc2 = \"It is one of the world's leading and most influential institutions of higher learning, with top-ten positions in numerous rankings and measures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('University', 'NNP'),\n",
       " ('of', 'IN'),\n",
       " ('Chicago', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('private', 'JJ'),\n",
       " ('research', 'NN'),\n",
       " ('university', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('Chicago', 'NNP'),\n",
       " (',', ','),\n",
       " ('Illinois', 'NNP')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.tokenize.word_tokenize(uc1)\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('one', 'CD'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('world', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('leading', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('most', 'JJS'),\n",
       " ('influential', 'JJ'),\n",
       " ('institutions', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('higher', 'JJR'),\n",
       " ('learning', 'NN'),\n",
       " (',', ','),\n",
       " ('with', 'IN'),\n",
       " ('top-ten', 'JJ'),\n",
       " ('positions', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('numerous', 'JJ'),\n",
       " ('rankings', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('measures', 'NNS')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.tokenize.word_tokenize(uc2)\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from URL\n",
    "#### BeautifulSoup to clean up meta-tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Harvard_University\"\n",
    "\n",
    "# wget -E  -k -p https://en.wikipedia.org/wiki/University_of_Chicago\n",
    "#url_saved = '/project/msca/kadochnikov/wiki/en.wikipedia.org/wiki/University_of_Chicago.html'\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "page = urllib.request.urlopen(url)\n",
    "soup = BeautifulSoup(page.read(), \"lxml\")\n",
    "\n",
    "#with open(url_saved, 'rb') as html:\n",
    "#    soup = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on, he was motivated not by a desire to secularize education but by Transcendentalist Unitarian convictions influenced by William Ellery Channing and Ralph Waldo Emerson.[29]\n",
      "\n",
      "20th century\n",
      " Richard Rummell's 1906 watercolor landscape view, facing northeast.[30]\n",
      "In the 20th century, Harvard's reputation grew as a burgeoning endowment and prominent professors expanded the university's scope. Rapid enrollment growth continued as new graduate schools were begun and the undergraduate college expanded. Radcliffe College, established in 1879 as the female counterpart of Harvard College, became one of the most prominent schools for women in the United States. Harvard became a founding member of the Association of American Universities in 1900.[10]\n",
      "The student body in the early decades of the century was predominantly \"old-stock, high-status Protestants, especially Episcopalians, Congregationalists, and Presbyterians.\" A 1923 proposal by President A. Lawrence Lowell that Jews be limited to 15% of undergraduates was rejected, but Lowell did ban blacks from freshman dormitories.[31][32][33][34]\n",
      "President James B. Conant reinvigorated creative scholarship to guarantee Harvard's preeminence among research institutions. He saw higher education as a vehicle of opportunity for the talented rather than an entitlement for the wealthy, so Conant devised programs to identify, recruit, and support talented youth. In 1943, he asked the faculty to make a definitive statement about what general education ought to be, at the secondary as well as at the college level. The resulting Report, published in 1945, was one of the most influential manifestos in 20th century American education.[35]\n",
      "Between 1945 and 1960, admissions were opened up to bring in a more diverse group of students. No longer drawing mostly from select New England prep schools, the undergraduate college became accessible to striving middle class students from public schools; many more Jews and Catholics were admitted, but few blacks, Hispanics, or Asians.[36]\n",
      "Throughout the rest of the 20th century, Harvard becam\n"
     ]
    }
   ],
   "source": [
    "uc_wiki = (soup.get_text())\n",
    "#print (type(uc_wiki))\n",
    "print (uc_wiki[6910:9000]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Even after BeautifulSoup we are left with a lot of garbade - mostly punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'s\", 'expansion', 'in', 'Allston', ',', 'Massachusetts', 'Harvard', 'Business', 'School', ',', 'Harvard', 'Innovation', 'Labs', ',', 'and', 'many', 'athletics', 'facilities', ',', 'including', 'Harvard', 'Stadium', ',', 'are', 'located', 'on', 'a', '358-acre', '(', '145', 'ha', ')', 'campus', 'in', 'Allston', ',', '[', '48', ']', 'a', 'Boston', 'neighborhood', 'just', 'across', 'the', 'Charles', 'River', 'from', 'the', 'Cambridge', 'campus', '.', 'The', 'John', 'W.', 'Weeks', 'Bridge', ',', 'a', 'pedestrian', 'bridge', 'over', 'the', 'Charles', 'River', ',', 'connects', 'the', 'two', 'campuses', '.', 'The', 'university', 'is', 'actively', 'expanding', 'into', 'Allston', ',', 'where', 'it', 'now', 'owns', 'more', 'land', 'than', 'in', 'Cambridge', '.', '[', '49', ']', 'Plans', 'include', 'new', 'construction', 'and', 'renovation', 'for', 'the']\n"
     ]
    }
   ],
   "source": [
    "uc_wiki_tokens = nltk.tokenize.word_tokenize(uc_wiki)\n",
    "uc_wiki_tokens_uncleaned = uc_wiki_tokens\n",
    "print (uc_wiki_tokens[2000:2100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Write a simple code to clean data of the scraped text.\n",
    "\n",
    "You will need to create:\n",
    "\n",
    "1. stopwords - using nltk corpus\n",
    "2. uc_wiki_tokens - remove single-character tokens and punctuation\n",
    "3. uc_wiki_tokens_no_stopwords - Remove stopwords\n",
    "4. fdist - run function __FreqDist__ on uc_wiki_tokens_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Harvard', 336),\n",
       " ('University', 108),\n",
       " ('The', 106),\n",
       " ('Retrieved', 101),\n",
       " ('College', 84),\n",
       " ('Boston', 52),\n",
       " ('September', 51),\n",
       " ('School', 35),\n",
       " ('John', 34),\n",
       " ('New', 31)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stopwords = stopwords.words('english')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Remove single-character tokens (mostly punctuation)\n",
    "uc_wiki_tokens = [word for word in uc_wiki_tokens if len(word) > 1]\n",
    "\n",
    "# Remove punctuation\n",
    "uc_wiki_tokens = [word for word in uc_wiki_tokens if word.isalpha()]\n",
    "\n",
    "# Remove stopwords\n",
    "uc_wiki_tokens_no_stopwords = [word for word in uc_wiki_tokens if word not in stopwords]\n",
    "\n",
    "fdist = nltk.FreqDist(uc_wiki_tokens_no_stopwords)\n",
    "\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how __The__ made it to the list even though stop words were removed. This is because __The__ starts with a captial T which makes it a proper noun. We can fix this by lower casing the tokens but then we would lose out on our POS taggin of proper nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the results of our cleaned web scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanest version with all noise and stopwords removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Harvard', 'University', 'Wikipedia', 'Harvard', 'University', 'From', 'Wikipedia', 'free', 'encyclopedia', 'Jump', 'navigation', 'Jump', 'search', 'Private', 'research', 'university', 'Cambridge', 'Massachusetts', 'United', 'States', 'Harvard', 'redirects', 'For', 'uses', 'see', 'Harvard', 'disambiguation', 'It', 'suggested', 'Harvard', 'University', 'Health', 'Services', 'merged', 'article', 'Discuss', 'Proposed', 'since', 'September', 'Harvard', 'UniversityCoat', 'armsLatin', 'Universitas', 'HarvardianaFormer', 'namesHarvard', 'CollegeMottoVeritas', 'Latin', 'Motto', 'years', 'ago', 'Endowment', 'billion', 'PresidentLawrence', 'BacowAcademic', 'faculty', 'members', 'academic', 'appointments', 'affiliated', 'teaching', 'hospitals', 'Fall', 'Fall', 'Fall', 'LocationCambridge', 'Massachusetts', 'United', 'acres', 'ha', 'NewspaperThe', 'Harvard', 'CrimsonColors', 'Crimson', 'AthleticsNCAA', 'Division', 'Ivy', 'LeagueNicknameHarvard', 'Harvard', 'University', 'private', 'Ivy', 'League', 'research', 'university', 'Cambridge', 'Massachusetts', 'Established', 'named', 'first', 'benefactor', 'clergyman', 'John', 'Harvard', 'Harvard', 'oldest', 'institution', 'higher', 'learning', 'United', 'States']\n"
     ]
    }
   ],
   "source": [
    "uc_wiki_text_no_stopwords = nltk.Text(uc_wiki_tokens_no_stopwords)\n",
    "print (uc_wiki_text_no_stopwords[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modest cleaning - only punctuation and noise - all stopwords left intact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Harvard', 'University', 'Wikipedia', 'Harvard', 'University', 'From', 'Wikipedia', 'the', 'free', 'encyclopedia', 'Jump', 'to', 'navigation', 'Jump', 'to', 'search', 'Private', 'research', 'university', 'in', 'Cambridge', 'Massachusetts', 'United', 'States', 'Harvard', 'redirects', 'here', 'For', 'other', 'uses', 'see', 'Harvard', 'disambiguation', 'It', 'has', 'been', 'suggested', 'that', 'Harvard', 'University', 'Health', 'Services', 'be', 'merged', 'into', 'this', 'article', 'Discuss', 'Proposed', 'since', 'September', 'Harvard', 'UniversityCoat', 'of', 'armsLatin', 'Universitas', 'HarvardianaFormer', 'namesHarvard', 'CollegeMottoVeritas', 'Latin', 'Motto', 'in', 'years', 'ago', 'Endowment', 'billion', 'PresidentLawrence', 'BacowAcademic', 'faculty', 'members', 'and', 'academic', 'appointments', 'in', 'affiliated', 'teaching', 'hospitals', 'Fall', 'Fall', 'Fall', 'LocationCambridge', 'Massachusetts', 'United', 'acres', 'ha', 'NewspaperThe', 'Harvard', 'CrimsonColors', 'Crimson', 'AthleticsNCAA', 'Division', 'Ivy', 'LeagueNicknameHarvard', 'Harvard', 'University', 'is', 'private', 'Ivy', 'League', 'research']\n"
     ]
    }
   ],
   "source": [
    "uc_wiki_text_cleaned = nltk.Text(uc_wiki_tokens)\n",
    "print (uc_wiki_text_cleaned[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No cleaning done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Harvard', 'University', '-', 'Wikipedia', 'Harvard', 'University', 'From', 'Wikipedia', ',', 'the', 'free', 'encyclopedia', 'Jump', 'to', 'navigation', 'Jump', 'to', 'search', 'Private', 'research', 'university', 'in', 'Cambridge', ',', 'Massachusetts', ',', 'United', 'States', \"''\", 'Harvard', \"''\", 'redirects', 'here', '.', 'For', 'other', 'uses', ',', 'see', 'Harvard', '(', 'disambiguation', ')', '.', 'It', 'has', 'been', 'suggested', 'that', 'Harvard', 'University', 'Health', 'Services', 'be', 'merged', 'into', 'this', 'article', '.', '(', 'Discuss', ')', 'Proposed', 'since', 'September', '2020', '.', 'Harvard', 'UniversityCoat', 'of', 'armsLatin', ':', 'Universitas', 'HarvardianaFormer', 'namesHarvard', 'CollegeMottoVeritas', '(', 'Latin', ')', '[', '1', ']', 'Motto', 'in', 'EnglishTruthTypePrivateEstablished1636', ';', '384', 'years', 'ago', '(', '1636', ')', '[', '2', ']', 'Endowment', '$', '41.9', 'billion', '(']\n"
     ]
    }
   ],
   "source": [
    "uc_wiki_text_raw = nltk.Text(uc_wiki_tokens_uncleaned)\n",
    "print (uc_wiki_text_raw[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying simlarity function - which option produces best results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Write a simple code to run similar function on:\n",
    "1. uc_wiki_text_no_stopwords\n",
    "2. uc_wiki_text_cleaned\n",
    "3. uc_wiki_text_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanest version with all noise and stopwords removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "college gazette harvard crimson graduate cambridge among modern\n",
      "universities students reputation report hall square collection\n",
      "shanghai board number subject spot\n"
     ]
    }
   ],
   "source": [
    "uc_wiki_text_no_stopwords.similar('university')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modest cleaning - only punctuation and noise - all stopwords left intact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harvard college gazette endowment crimson as graduate center number to\n",
      "has of faculty and academic division is among world founding\n"
     ]
    }
   ],
   "source": [
    "uc_wiki_text_cleaned.similar('university')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No cleaning done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harvard college gazette endowment crimson graduate center number to\n",
      "has of faculty and academic division is among world founding as\n"
     ]
    }
   ],
   "source": [
    "uc_wiki_text_raw.similar('university')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seems that no cleaning provided us with the best result. That is because we remove a lot of context based tokens when we clean aggresively.\n",
    "\n",
    "In this case, what we see are the two critical pieces. If we prepare something for human consumption. If we want to summarize the text and presented for our customers or for executives to read. We perform the the most aggressive cleanup.\n",
    "\n",
    "If we prepare the text as the features for machine learning or any automated algorithms like supervised models built on NLP, we cannot afford to do any cleanups at all, or we mess up the context and the models do not work as intended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging our web page with POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order for the tagger to be effective, it has to tag each word based on the word itself, as well as its context within a sentence. \n",
    "Depending on your corpus, certain taggers perform better the others.  Like with SPSS TA dictionaries, you can start with pre-trained POS Tagger and then try multiple different options to see which one will perform best for you.\n",
    "You can also customize and train your own taggers to match your particular corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "uc_wiki_tagged = nltk.pos_tag(uc_wiki_text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Harvard', 'NNP'),\n",
       " ('University', 'NNP'),\n",
       " ('Wikipedia', 'NNP'),\n",
       " ('Harvard', 'NNP'),\n",
       " ('University', 'NNP'),\n",
       " ('From', 'NNP'),\n",
       " ('Wikipedia', 'NNP'),\n",
       " ('the', 'DT'),\n",
       " ('free', 'JJ'),\n",
       " ('encyclopedia', 'NN')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uc_wiki_tagged[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring alternative text analysis packages: TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(uc_wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(']', 'VBD'),\n",
       " ('The', 'DT'),\n",
       " ('Massachusetts', 'NNP'),\n",
       " ('colonial', 'JJ'),\n",
       " ('legislature', 'NN'),\n",
       " ('the', 'DT'),\n",
       " ('General', 'NNP'),\n",
       " ('Court', 'NNP'),\n",
       " ('authorized', 'VBD'),\n",
       " ('Harvard', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('founding', 'NN'),\n",
       " ('In', 'IN'),\n",
       " ('its', 'PRP$'),\n",
       " ('early', 'JJ'),\n",
       " ('years', 'NNS'),\n",
       " ('Harvard', 'NNP'),\n",
       " ('College', 'NNP'),\n",
       " ('primarily', 'RB'),\n",
       " ('trained', 'VBD')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.tags[200:220]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To process  cleaned-up version from NLTK we will have to convert text from nltk.text.Text to String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "words_list = (uc_wiki_text_cleaned[0:])\n",
    "words_string = ', '.join(words)\n",
    "print(type(words_list))\n",
    "print(type(words_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(words_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS Tagging with TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TO', 'NNP'),\n",
       " ('ONE', 'NNP'),\n",
       " ('THERE', 'NNP'),\n",
       " ('were', 'VBD'),\n",
       " ('four', 'CD'),\n",
       " ('of', 'IN'),\n",
       " ('us', 'PRP'),\n",
       " ('George', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('William', 'NNP'),\n",
       " ('Samuel', 'NNP'),\n",
       " ('Harris', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('myself', 'PRP'),\n",
       " ('and', 'CC'),\n",
       " ('Montmorency', 'NNP'),\n",
       " ('We', 'PRP'),\n",
       " ('were', 'VBD'),\n",
       " ('sitting', 'VBG'),\n",
       " ('in', 'IN')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.tags[70:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('four', 'CD'),\n",
       " ('of', 'IN'),\n",
       " ('us', 'PRP'),\n",
       " ('George', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('William', 'NNP'),\n",
       " ('Samuel', 'NNP'),\n",
       " ('Harris', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('myself', 'PRP'),\n",
       " ('and', 'CC'),\n",
       " ('Montmorency', 'NNP'),\n",
       " ('We', 'PRP'),\n",
       " ('were', 'VBD'),\n",
       " ('sitting', 'VBG'),\n",
       " ('in', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('room', 'NN'),\n",
       " ('smoking', 'NN'),\n",
       " ('and', 'CC')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(bk_3boat)\n",
    "words_string = ', '.join(words)\n",
    "blob = TextBlob(words_string)\n",
    "\n",
    "blob.tags[80:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tripping on the capitalized header of the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('THREE', 'CD'),\n",
       " ('MEN', 'NNP'),\n",
       " ('IN', 'NNP'),\n",
       " ('A', 'NNP'),\n",
       " ('BOAT', 'NNP'),\n",
       " ('TO', 'NNP'),\n",
       " ('SAY', 'NNP'),\n",
       " ('NOTHING', 'NNP'),\n",
       " ('OF', 'NNP'),\n",
       " ('THE', 'NNP')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.tags[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that because of capitilization on the name of the book the POS tagger confused it for proper nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['three', 'men', 'in', 'boat', 'to', 'say', 'nothing', 'of', 'the', 'dog', 'men', 'boat', 'jerome', 'k.', 'jerome', 'chapter', 'three', 'invalids', 'sufferings', 'of', 'george', 'and', 'harris', 'victim', 'to', 'one', 'hundred', 'and', 'seven', 'fatal', 'maladies', 'useful', 'prescriptions', 'cure', 'for', 'liver', 'complaint', 'in', 'children', 'we', 'agree', 'that', 'we', 'are', 'overworked', 'and', 'need', 'rest', 'week', 'on'])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.noun_phrases[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we get about nouns and proper nouns in a text is because that's usually truly shows us the subjects and objects of the text. So you can quickly just get rid of everything else.So if we just look at the top 50 nouns in a text as a given us some information about the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also have embedded functions to pluralize and singularize the words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = TextBlob(words_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['three', 'men', 'boat', 'say', 'nothing', 'dog', 'three', 'men', 'boat', 'jerome', 'jerome', 'chapter', 'three', 'invalids', 'sufferings', 'george', 'harris', 'victim', 'one', 'hundred', 'seven', 'fatal', 'maladies', 'useful', 'prescriptions', 'cure', 'liver', 'complaint', 'children', 'agree', 'overworked', 'need', 'rest', 'week', 'rolling', 'deep', 'george', 'suggests', 'river', 'montmorency', 'lodges', 'objection', 'original', 'motion', 'carried', 'majority', 'three', 'one', 'four', 'us']\n"
     ]
    }
   ],
   "source": [
    "print(words_l[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['three', 'man', 'boat', 'say', 'nothing', 'dog', 'three', 'man', 'boat', 'jerome', 'jerome', 'chapter', 'three', 'invalid', 'suffering', 'george', 'harri', 'victim', 'one', 'hundred', 'seven', 'fatal', 'malady', 'useful', 'prescription', 'cure', 'liver', 'complaint', 'child', 'agree', 'overworked', 'need', 'rest', 'week', 'rolling', 'deep', 'george', 'suggest', 'river', 'montmorency', 'lodge', 'objection', 'original', 'motion', 'carried', 'majority', 'three', 'one', 'fmy', 'u'])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.words[:100].singularize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['three', 'men', 'boat', 'say', 'nothing', 'dog', 'three', 'men', 'boat', 'jerome', 'jerome', 'chapter', 'three', 'invalids', 'sufferings', 'george', 'harris', 'victim', 'one', 'hundred', 'seven', 'fatal', 'maladies', 'useful', 'prescriptions', 'cure', 'liver', 'complaint', 'children', 'agree', 'overworked', 'need', 'rest', 'week', 'rolling', 'deep', 'george', 'suggests', 'river', 'montmorency', 'lodges', 'objection', 'original', 'motion', 'carried', 'majority', 'three', 'one', 'four', 'us']\n"
     ]
    }
   ],
   "source": [
    "print(words_l[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['threes', 'mens', 'boats', 'says', 'nothings', 'dogs', 'threes', 'mens', 'boats', 'jeromes', 'jeromes', 'chapters', 'threes', 'invalidss', 'sufferingss', 'georges', 'harriss', 'victims', 'ones', 'hundreds', 'sevens', 'fatals', 'maladiess', 'usefuls', 'prescriptionss', 'cures', 'livers', 'complaints', 'childrens', 'agrees', 'overworkeds', 'needs', 'rests', 'weeks', 'rollings', 'deeps', 'georges', 'suggestss', 'rivers', 'montmorencies', 'lodgess', 'objections', 'originals', 'motions', 'carrieds', 'majorities', 'threes', 'ones', 'fours', 'uss'])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.words[0:100].pluralize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pluralize and singularize do not do a good job using the NLTK implimentation hence should be avoided most of the times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['29', '20th', 'century', 'Richard', 'Rummell', \"'s\", '1906', 'watercolor', 'landscape', 'view', 'facing', 'northeast', '30', 'In', 'the', '20th', 'century', 'Harvard', \"'s\", 'reputation', 'grew', 'as', 'a', 'burgeoning', 'endowment', 'and', 'prominent', 'professors', 'expanded', 'the', 'university', \"'s\", 'scope', 'Rapid', 'enrollment', 'growth', 'continued', 'as', 'new', 'graduate', 'schools', 'were', 'begun', 'and', 'the', 'undergraduate', 'college', 'expanded', 'Radcliffe', 'College', 'established', 'in', '1879', 'as', 'the', 'female', 'counterpart', 'of', 'Harvard', 'College']\n"
     ]
    }
   ],
   "source": [
    "blob_wiki = TextBlob(uc_wiki)\n",
    "b_words = blob_wiki.words\n",
    "print (b_words[1020:1080])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'class', '1.1.3', 'is', 'almost', 'over', 'Time', 'to', 'go', 'party', 'now']\n"
     ]
    }
   ],
   "source": [
    "blob_custom = TextBlob('The class 1.1.3 is almost over!!!  Time to go party now.')\n",
    "b_words = blob_custom.words\n",
    "print (b_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence(\"[10] James B. Conant led the university through the Great Depression and World War II; he liberalized admissions after the war.\"), Sentence(\"The university is composed of ten academic faculties plus the Radcliffe Institute for Advanced Study.\"), Sentence(\"Arts and Sciences offers study in a wide range of academic disciplines for undergraduates and for graduates, while the other faculties offer only graduate degrees, mostly professional.\"), Sentence(\"Harvard has three main campuses:[11]\n",
      "the 209-acre (85 ha) Cambridge campus centered on Harvard Yard; an adjoining campus immediately across the Charles River in the Allston neighborhood of Boston; and the medical campus in Boston's Longwood Medical Area.\"), Sentence(\"[12] Harvard's endowment is valued at $41.9 billion, making it the largest of any academic institution.\")]\n"
     ]
    }
   ],
   "source": [
    "b_sentences = blob_wiki.sentences\n",
    "print (b_sentences[10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence(\"The class 1, 2, etc.\"), Sentence(\"are almost over!!!\"), Sentence(\"Time to go to Dr. Yuri's party 1.2. now.\")]\n"
     ]
    }
   ],
   "source": [
    "blob_custom = TextBlob('''The class 1, 2, etc. are almost over!!!  Time to go to Dr. Yuri's party 1.2. now.''')\n",
    "b_sentences = blob_custom.sentences\n",
    "print (b_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These methods are also useful to understand the context of the text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
