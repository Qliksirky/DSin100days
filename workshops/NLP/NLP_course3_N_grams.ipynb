{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language processing - Edit distance and spelling correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites for this course\n",
    "1. Basic Python Programming \n",
    "2. Basic Python Libraries\n",
    "\n",
    "Incase you do not have the necessary knowledge of these topics kindly check out our Refactored.ai course on the same:\n",
    "1. [Python for Data Scientists](https://refactored.ai/course/python-for-data-scientists/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Probabalistic Language Models__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular idea in computational linguistics is to create a probabilistic model of language. Such a model assigns a probability to every sentence in English in such a way that more likely sentences (in some sense) get higher probability. If we are unsure between two possible sentences, we pick the higher probability one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assign a probability to a sentence<br>\n",
    "\n",
    "* Machine Translation:\n",
    " * P(__high__ winds tonite ) > P(__large__ winds tonite)\n",
    "\n",
    "Even though high and large are spelled correctly they do fit the sentence the same way. Hence __high__ will be a better word to fit this sentence\n",
    "\n",
    "* Spell Correction\n",
    " * The office is about fifteen __minuets__ from my house\n",
    "   * P(about fifteen __minutes__ from) > P(about fifteen __minuets__ from)\n",
    "\n",
    "Even though __minuets__ and __minutes__ are spelled correctly they do fit the sentence the same way. __minutes__ make more sense in this sentence\n",
    "\n",
    "* Speech Recognition\n",
    " * P(I saw a van) >> P(eyes awe of an)\n",
    "<br>\n",
    "* Summarization, question answering, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: compute the probability of a sentence or sequence of words:<br>\n",
    "P(W) = P(w1 ,w2 ,w3 ,w4 ,w5.... wn)\n",
    "\n",
    "Related task: probability of an upcoming word:<br>\n",
    "P(w5 |w1 ,w2 ,w3 ,w4)\n",
    "\n",
    "A model that computes either of these:<br>\n",
    "P(W) or P(wn |w1 ,w2 …w n-1 ) is called a language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How to compute P(W)__\n",
    "* How to compute this joint probability:\n",
    "* Intuition: let’s rely on the Chain Rule of Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at Bigram models which can look a the probaility of a word occuring based on the previous word.\n",
    "We can extend to trigrams,4-grams,5-grams which are called __N-gram models__\n",
    "\n",
    "In general this is an insufficient model of language\n",
    "* because language has long distance dependencies\n",
    "\n",
    "* But we can often get away with N gram models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__N-Grams for Text Similarity Comparing text__\n",
    "\n",
    "Comparing text across multiple documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use distance measure techniques to do that\n",
    "\n",
    "* Goal: Find near neighbors in high dimensional space\n",
    " * We formally define “near neighbors” as points that are a “small distance” apart\n",
    "* For each application, we first need to define what “ distance ” means\n",
    "* __Jaccard distance/similarity__\n",
    " * The Jaccard similarity of two sets is the size of their intersection divided by the size of their union: \n",
    " * Jaccard distance is given by the formula displayed below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images\Jaccard_distance.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents as high dim data\n",
    "\n",
    "\n",
    "* Step 1: Is to perform N Gramming. N Gramming requires us to convert documents to sets\n",
    " * Simple approaches:\n",
    "   * Document = set of words appearing in document\n",
    "   * Document = set of “important” words\n",
    "   \n",
    "* Need to account for ordering of words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define N-Grams\n",
    "\n",
    "* An n gram for a document is a sequence of n tokens that appears in the doc\n",
    " * Tokens can be characters , words or something else, depending on the application\n",
    "\n",
    "\n",
    "* Example: __n=2__ ; document __D1 = abcab__ <br>Set of 2 grams: __S(D1)__ = {ab , bc , ca}\n",
    " * Option: n grams as a bag (multiset), count ab twice: __S’(D1)__ = ab , bc , ca,ab\n",
    " \n",
    " \n",
    "Hence to find similarity between documents we use Jaccard distance formula on the N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__WORKING A SSUMPTION__\n",
    "\n",
    "* Documents that have lots of n grams in common have similar text, even if the text appears in different order\n",
    "* Caveat: You must pick n large enough, or most documents will have most n grams\n",
    " * n = 5 is OK for short documents\n",
    " * n = 10 is better for long documents\n",
    " \n",
    "In practical user we need to take a small sample of our documents and we need to read them and we need to label them . For the documents have a very dissimilar in our perspective, we want the model to indicate that they're actually dissimilar and for the document similar we want them to be identified as similar using our model.\n",
    "Hence we use this method to find the sweet spot to identify the \"n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the following code\n",
    "import nltk as nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You cannot just run Jaccard Distance on the strings directly; you must first convert them to the set type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard Distance between charaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jaccard distance between commuter and computer\n",
    "nltk.jaccard_distance(set('commuter'), set('computer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The above value of 0.125 can be calculated using the Jaccard distance formula stated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.jaccard_distance(set('Commuter'), set('computer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jaccard distance is case sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.jaccard_distance(set('acorn'), set('corn'))\n",
    "#intersection 4 and union is 5 hence 4/5=0.8 and Jaccard distance=1-0.8=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.jaccard_distance(set('aacorn'), set('corn'))\n",
    "#As aacorn has 2 \"a\"s in it as we convert the text to set it becomes unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chicago 0.14\n",
      "university 0.93\n",
      "composed 0.82\n",
      "undergraduate 0.83\n",
      "college 0.62\n",
      "various 0.7\n",
      "graduate 0.82\n",
      "programs 0.7\n"
     ]
    }
   ],
   "source": [
    "word_1 = 'chicago'\n",
    "\n",
    "word_n = ['Chicago', 'university', 'composed', 'undergraduate', 'college', 'various',  'graduate', 'programs']\n",
    " \n",
    "for word in word_n:\n",
    "    jaccardDistance = nltk.jaccard_distance(set(word_1), set(word))\n",
    "    print(word, round(jaccardDistance,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Write a simple code to find Jaccard distance between \"super\" and \"sticky\":\n",
    "\n",
    "##### There are many useful python built-in functions pre-defined within the python environment that can be directly invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.jaccard_distance(set('super'), set('sticky'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Level\n",
    "\n",
    "Now let us look at word level Jaccard distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use the nltk.word_tokenize to split the sentences to words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 Jaccard Distance between sent1 and sent2\n",
      "0.0 Jaccard Distance between sent1 and sent3\n",
      "0.4 Jaccard Distance between sent1 and sent4\n",
      "0.4 Jaccard Distance between sent1 and sent5\n"
     ]
    }
   ],
   "source": [
    "sent1 = \"I love cookies\"\n",
    "sent2 = \"i love love love love love love love love love cookies\"\n",
    "sent3 = \"cookies I love\"\n",
    "sent4 = \"I love cookies with tea\"\n",
    "sent5 = \"I love tea with cookies\"\n",
    "\n",
    "wrds_sent1 = nltk.word_tokenize(sent1)\n",
    "wrds_sent2 = nltk.word_tokenize(sent2)\n",
    "wrds_sent3 = nltk.word_tokenize(sent3)\n",
    "wrds_sent4 = nltk.word_tokenize(sent4)\n",
    "wrds_sent5 = nltk.word_tokenize(sent5)\n",
    "\n",
    "jd_sent_1_2 = nltk.jaccard_distance(set(wrds_sent1), set(wrds_sent2))\n",
    "jd_sent_1_3 = nltk.jaccard_distance(set(wrds_sent1), set(wrds_sent3))\n",
    "jd_sent_1_4 = nltk.jaccard_distance(set(wrds_sent1), set(wrds_sent4))\n",
    "jd_sent_1_5 = nltk.jaccard_distance(set(wrds_sent1), set(wrds_sent5))\n",
    "\n",
    "print(jd_sent_1_2, 'Jaccard Distance between sent1 and sent2')\n",
    "print(jd_sent_1_3, 'Jaccard Distance between sent1 and sent3')\n",
    "print(jd_sent_1_4, 'Jaccard Distance between sent1 and sent4')\n",
    "print(jd_sent_1_5, 'Jaccard Distance between sent1 and sent5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution 1 is 0.5 beacuse of intersection \"love cookies\" -> 2 and union - \"I i love cookies\" -> 4, hence 2/4=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with Book and distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this code to read book\n",
    "from textblob import TextBlob\n",
    "book = '3boat10.txt'\n",
    "f = open(book)\n",
    "book_text = f.read()\n",
    "\n",
    "#use text blob to tokenize the book\n",
    "blob = TextBlob(book_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['only', 'one', 'or', 'two', 'diseases', 'each', 'So', 'I', 'went', 'straight', 'up', 'and', 'saw', 'him', 'and', 'he', 'said', 'Well', 'what', \"'s\", 'the', 'matter', 'with', 'you', 'I', 'said', 'I', 'will', 'not', 'take', 'up', 'your', 'time', 'dear', 'boy', 'with', 'telling', 'you', 'what', 'is', 'the', 'matter', 'with', 'me', 'Life', 'is', 'brief', 'and', 'you', 'might', 'pass', 'away', 'before', 'I', 'had', 'finished', 'But', 'I', 'will', 'tell']\n"
     ]
    }
   ],
   "source": [
    "#Let us look at some words\n",
    "b_words = blob.words\n",
    "print (b_words[1020:1080])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence(\"- \n",
      "MONTMORENCY LODGES AN OBJECTION.\"), Sentence(\"- ORIGINAL MOTION CARRIED BY MAJORITY OF \n",
      "THREE TO ONE.\"), Sentence(\"THERE were four of us - George, and William Samuel Harris, and myself, \n",
      "and Montmorency.\"), Sentence(\"We were sitting in my room, smoking, and talking about \n",
      "how bad we were - bad from a medical point of view I mean, of course.\"), Sentence(\"We were all feeling seedy, and we were getting quite nervous about it.\")]\n"
     ]
    }
   ],
   "source": [
    "#Let us look at some sentences\n",
    "b_sentences = blob.sentences\n",
    "print (b_sentences[10:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining prior, next and current words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import tee, islice, chain\n",
    "\n",
    "def previous_and_next(some_iterable):\n",
    "    prevs, items, nexts = tee(some_iterable, 3)\n",
    "    prevs = chain([None], prevs)\n",
    "    nexts = chain(islice(nexts, 1, None), [None])\n",
    "    return zip(prevs, items, nexts)\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item is now THREE next is MEN previous is None\n",
      "Item is now MEN next is IN previous is THREE\n",
      "Item is now IN next is A previous is MEN\n",
      "Item is now A next is BOAT previous is IN\n",
      "Item is now BOAT next is TO previous is A\n",
      "Item is now TO next is SAY previous is BOAT\n",
      "Item is now SAY next is NOTHING previous is TO\n",
      "Item is now NOTHING next is OF previous is SAY\n",
      "Item is now OF next is THE previous is NOTHING\n",
      "Item is now THE next is DOG previous is OF\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for previous, item, nxt in previous_and_next(b_words):\n",
    "    print (\"Item is now\", item, \"next is\", nxt, \"previous is\", previous)\n",
    "    count += 1\n",
    "    if count >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous set of code is something similar we did earlier in looking at concordance. Over here we are trying to look at what values (probabilites) a word has when it fits between \"Item is now __(blank)__ next is __(blank)__ previous is __(blank)__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard distance betwen current and next words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Distance between:  THREE &  MEN is:  0.8333333333333334\n",
      "Jaccard Distance between:  MEN &  IN is:  0.75\n",
      "Jaccard Distance between:  IN &  A is:  1.0\n",
      "Jaccard Distance between:  A &  BOAT is:  0.75\n",
      "Jaccard Distance between:  BOAT &  TO is:  0.5\n",
      "Jaccard Distance between:  TO &  SAY is:  1.0\n",
      "Jaccard Distance between:  SAY &  NOTHING is:  1.0\n",
      "Jaccard Distance between:  NOTHING &  OF is:  0.8571428571428571\n",
      "Jaccard Distance between:  OF &  THE is:  1.0\n",
      "Jaccard Distance between:  THE &  DOG is:  1.0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for previous, item, nxt in previous_and_next(b_words):\n",
    "    print (\"Jaccard Distance between: \", item, \"& \", nxt, \"is: \", nltk.jaccard_distance(set(item),set(nxt)))\n",
    "    count += 1\n",
    "    if count >= 10:\n",
    "        break\n",
    "        \n",
    "# migt be able use for spell check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard distance betwen current and next sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Distance between:  THREE MEN IN A BOAT\n",
      "(TO SAY NOTHING OF THE DOG). &  Three Men in a Boat by Jerome K. Jerome\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CHAPTER I. is:  0.6857142857142857\n",
      "Jaccard Distance between:  Three Men in a Boat by Jerome K. Jerome\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CHAPTER I. &  THREE INVALIDS. is:  0.7419354838709677\n",
      "Jaccard Distance between:  THREE INVALIDS. &  - SUFFERINGS OF GEORGE AND HARRIS. is:  0.4444444444444444\n",
      "Jaccard Distance between:  - SUFFERINGS OF GEORGE AND HARRIS. &  - A VICTIM TO ONE \n",
      "HUNDRED AND SEVEN FATAL MALADIES. is:  0.3333333333333333\n",
      "Jaccard Distance between:  - A VICTIM TO ONE \n",
      "HUNDRED AND SEVEN FATAL MALADIES. &  - USEFUL PRESCRIPTIONS. is:  0.3333333333333333\n",
      "Jaccard Distance between:  - USEFUL PRESCRIPTIONS. &  - CURE FOR \n",
      "LIVER COMPLAINT IN CHILDREN. is:  0.3333333333333333\n",
      "Jaccard Distance between:  - CURE FOR \n",
      "LIVER COMPLAINT IN CHILDREN. &  - WE AGREE THAT WE ARE OVERWORKED, AND NEED \n",
      "REST. is:  0.48\n",
      "Jaccard Distance between:  - WE AGREE THAT WE ARE OVERWORKED, AND NEED \n",
      "REST. &  - A WEEK ON THE ROLLING DEEP? is:  0.4090909090909091\n",
      "Jaccard Distance between:  - A WEEK ON THE ROLLING DEEP? &  - GEORGE SUGGESTS THE RIVER. is:  0.5714285714285714\n",
      "Jaccard Distance between:  - GEORGE SUGGESTS THE RIVER. &  - \n",
      "MONTMORENCY LODGES AN OBJECTION. is:  0.5652173913043478\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for previous, item, nxt in previous_and_next(b_sentences):\n",
    "    print (\"Jaccard Distance between: \", item, \"& \", nxt, \"is: \", nltk.jaccard_distance(set(item),set(nxt)))\n",
    "    count += 1\n",
    "    if count >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application of these two methods can be to cluster similar and dissimilar comments from something like tweets to then later process them for future NLP applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N Grams and Tokenization in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approaching Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = '3boat10.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 3607),\n",
       " ('and', 3395),\n",
       " ('to', 1790),\n",
       " ('a', 1714),\n",
       " ('of', 1496),\n",
       " ('it', 1422),\n",
       " ('i', 1213),\n",
       " ('in', 977),\n",
       " ('that', 950),\n",
       " ('he', 920)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the N most common words in book\n",
    "top_N = 10\n",
    "\n",
    "words = re.findall(r'\\w+', open(book, encoding=\"utf8\").read().lower())\n",
    "# \\w -- matches a \"word\" character: a letter or digit or underbar [a-zA-Z0-9_].\n",
    "# + pattern must appear at least once. \n",
    "\n",
    "word_freq = Counter(words).most_common(top_N)\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another way to capture the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>3607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>3395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>1496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>1422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>1213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Frequency\n",
       "Word           \n",
       "the        3607\n",
       "and        3395\n",
       "to         1790\n",
       "a          1714\n",
       "of         1496\n",
       "it         1422\n",
       "i          1213\n",
       "in          977\n",
       "that        950\n",
       "he          920"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "word_freq = Counter(words).most_common()\n",
    "\n",
    "word_freq_df = pd.DataFrame(word_freq,\n",
    "                    columns=['Word', 'Frequency']).set_index('Word')\n",
    "\n",
    "word_freq_df.sort_values('Frequency', ascending=False, inplace=True)\n",
    "\n",
    "word_freq_df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLTK to get the most common tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 7773 samples and 79643 outcomes>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(',', 5702),\n",
       " ('the', 3338),\n",
       " ('and', 3215),\n",
       " ('.', 3081),\n",
       " ('to', 1748),\n",
       " ('a', 1621),\n",
       " ('of', 1425),\n",
       " ('I', 1208),\n",
       " ('it', 1159),\n",
       " ('in', 931)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the file\n",
    "f = open(book, encoding=\"utf8\")\n",
    "raw = f.read()\n",
    "\n",
    "#using NLTK tokenizer\n",
    "words = nltk.tokenize.word_tokenize(raw)\n",
    "fdist = nltk.FreqDist(words)\n",
    "\n",
    "print(fdist)\n",
    "\n",
    "#fdist.items() - will give all words\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK also has embedded RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Flu',\n",
       " 'season',\n",
       " 'hitting',\n",
       " 'earlier',\n",
       " 'with',\n",
       " 'dozens',\n",
       " 'more',\n",
       " 'outbreaks',\n",
       " 'and',\n",
       " 'more',\n",
       " 'severe',\n",
       " 'symptoms']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer.tokenize(\"Flu season hitting earlier, with dozens more outbreaks — and more severe symptoms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N Grams\n",
    "### Basic N-Gramming using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the nltk.ngrams function present in the NLTK library to split the text into n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('quick', 'brown', 'fox')\n",
      "('brown', 'fox', 'jumps')\n",
      "('fox', 'jumps', 'over')\n",
      "('jumps', 'over', 'the')\n",
      "('over', 'the', 'lazy')\n",
      "('the', 'lazy', 'dog')\n"
     ]
    }
   ],
   "source": [
    "sentence = 'quick brown fox jumps over the lazy dog'\n",
    "n = 3\n",
    "kgrams = nltk.ngrams(sentence.split(), n)\n",
    "for grams in kgrams:\n",
    "    print (grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Write a simple code to find n-grams for the sentence \"NLP is super interesting\" and keep n=2:\n",
    "\n",
    "##### There are many useful python built-in functions pre-defined within the python environment that can be directly invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NLP', 'is')\n",
      "('is', 'super')\n",
      "('super', 'interesting')\n"
     ]
    }
   ],
   "source": [
    "sentence = 'NLP is super interesting'\n",
    "n = 2\n",
    "kgrams = nltk.ngrams(sentence.split(), n)\n",
    "for grams in kgrams:\n",
    "    print (grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK has inbuilt functions for Bi-gram and Tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "#Create your bigrams or trigrams\n",
    "bgs = nltk.bigrams(tokens)\n",
    "tgs = nltk.trigrams(tokens)\n",
    "\n",
    "#compute frequency distribution for all the bigrams in the text\n",
    "fdist_2 = nltk.FreqDist(bgs)\n",
    "fdist_3 = nltk.FreqDist(tgs)\n",
    "\n",
    "#for k,v in fdist.items():\n",
    "#    print (k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37517, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(,, and)</td>\n",
       "      <td>1859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(of, the)</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(., I)</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(in, the)</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(;, and)</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(., We)</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(., It)</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(., ``)</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(,, '')</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(and, the)</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word  Frequency\n",
       "0    (,, and)       1859\n",
       "1   (of, the)        318\n",
       "2      (., I)        293\n",
       "3   (in, the)        278\n",
       "4    (;, and)        233\n",
       "5     (., We)        227\n",
       "6     (., It)        220\n",
       "7     (., ``)        207\n",
       "8     (,, '')        182\n",
       "9  (and, the)        180"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_df = pd.DataFrame(fdist_2.most_common(),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "\n",
    "print(fdist_df.shape)\n",
    "\n",
    "fdist_df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see the major word distribution is of punctuation but if we move towards the middle of the list the results would make more sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>(from, affectation)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>(affectation, -)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>(often, wished)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>(able, .)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>(us, anecdotes)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Word  Frequency\n",
       "10000  (from, affectation)          1\n",
       "10001     (affectation, -)          1\n",
       "10002      (often, wished)          1\n",
       "10003            (able, .)          1\n",
       "10004      (us, anecdotes)          1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_df.iloc[10000:10005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning-up  N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminating puctuation and case sensitivity from N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens = nltk.tokenize.word_tokenize(raw)\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "#stopwords = stopwords.words('english')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "word_list = []\n",
    "\n",
    "# Filter out words that have punctuation and make everything lower-case\n",
    "cleaned_words = [w.lower() for w in tokens if w.isalnum()]\n",
    "\n",
    "bgs = [b for b in nltk.bigrams(cleaned_words) if b[0] not in stopwords and b[1] not in stopwords]\n",
    "tgs = [b for b in nltk.trigrams(cleaned_words) if b[0] not in stopwords and b[1] not in stopwords and b[2] not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking into top 10 bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(harris, said)</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(george, said)</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(said, george)</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(would, go)</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(one, another)</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(five, minutes)</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(said, oh)</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(said, harris)</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(young, men)</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(would, come)</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Word  Frequency\n",
       "0   (harris, said)         41\n",
       "1   (george, said)         34\n",
       "2   (said, george)         22\n",
       "3      (would, go)         17\n",
       "4   (one, another)         17\n",
       "5  (five, minutes)         16\n",
       "6       (said, oh)         16\n",
       "7   (said, harris)         15\n",
       "8     (young, men)         14\n",
       "9    (would, come)         13"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_2 = nltk.FreqDist(bgs)\n",
    "fdist_df = pd.DataFrame(fdist_2.most_common(),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "\n",
    "fdist_df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking into top 10 trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(herr, slossenn, boschen)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(two, young, men)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(uncle, podger, would)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(said, oh, yes)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(five, minutes, afterwards)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(got, housemaid, knee)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(harris, said, oh)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(magna, charta, island)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(double, sculling, skiff)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(two, lovely, black)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Word  Frequency\n",
       "0    (herr, slossenn, boschen)          7\n",
       "1            (two, young, men)          6\n",
       "2       (uncle, podger, would)          4\n",
       "3              (said, oh, yes)          4\n",
       "4  (five, minutes, afterwards)          4\n",
       "5       (got, housemaid, knee)          3\n",
       "6           (harris, said, oh)          3\n",
       "7      (magna, charta, island)          3\n",
       "8    (double, sculling, skiff)          3\n",
       "9         (two, lovely, black)          3"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_3 = nltk.FreqDist(tgs)\n",
    "fdist_df = pd.DataFrame(fdist_3.most_common(),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "\n",
    "fdist_df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating targeted N-Grams\n",
    "\n",
    "As the previous methods do not produce outputs which are great for human consumption we will look at targeted N-Grams apprach for a charater in the book we know is called \"harris\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens = nltk.tokenize.word_tokenize(raw)\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "#stopwords = stopwords.words('english')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "word_list = []\n",
    "\n",
    "# Filter out words that have punctuation and make everything lower-case\n",
    "cleaned_words = [w.lower() for w in tokens if w.isalnum()]\n",
    "\n",
    "bgs = [b for b in nltk.bigrams(cleaned_words) if b[0] not in stopwords and b[1] not in stopwords and \\\n",
    "       (b[0] == 'harris' or b[1] == 'harris')]\n",
    "\n",
    "tgs = [b for b in nltk.trigrams(cleaned_words) if b[0] not in stopwords and b[1] not in stopwords and \\\n",
    "       (b[0] == 'harris' or b[1] == 'harris' or b[2] == 'harris')]\n",
    "\n",
    "# Harris must be one of the words----shows the things that harris did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(harris, said)</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(said, harris)</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(harris, would)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(harris, told)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(george, harris)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(harris, never)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(time, harris)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(met, harris)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(harris, sat)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(cried, harris)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Word  Frequency\n",
       "0    (harris, said)         41\n",
       "1    (said, harris)         15\n",
       "2   (harris, would)          5\n",
       "3    (harris, told)          4\n",
       "4  (george, harris)          4\n",
       "5   (harris, never)          3\n",
       "6    (time, harris)          3\n",
       "7     (met, harris)          3\n",
       "8     (harris, sat)          3\n",
       "9   (cried, harris)          3"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_2 = nltk.FreqDist(bgs)\n",
    "fdist_df = pd.DataFrame(fdist_2.most_common(),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "\n",
    "fdist_df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(harris, said, he)</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(harris, said, that)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(harris, said, it)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(said, harris, and)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(harris, said, oh)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(ago, harris, said)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(harris, said, how)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(harris, sat, on)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(said, harris, then)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(harris, said, i)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Word  Frequency\n",
       "0    (harris, said, he)         11\n",
       "1  (harris, said, that)          7\n",
       "2    (harris, said, it)          5\n",
       "3   (said, harris, and)          4\n",
       "4    (harris, said, oh)          3\n",
       "5   (ago, harris, said)          2\n",
       "6   (harris, said, how)          2\n",
       "7     (harris, sat, on)          2\n",
       "8  (said, harris, then)          2\n",
       "9     (harris, said, i)          2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_3 = nltk.FreqDist(tgs)\n",
    "fdist_df = pd.DataFrame(fdist_3.most_common(),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "\n",
    "fdist_df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These methods can help us identify certain trends with certain word association"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Correction and the Noisy Channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application of Spelling correction can be seen everywhere in the present world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of spelling errors\n",
    "\n",
    "* Non word Errors\n",
    " * graffe -> giraffe\n",
    "* Real word Errors\n",
    " * Typographical errors\n",
    "   * three -> there\n",
    " * Cognitive Errors (homophones)\n",
    "   * piece -> peace ,\n",
    "   * too -> two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-word spelling errors\n",
    "\n",
    "* Non word spelling error detection:\n",
    " * Any word not in a dictionary is an error\n",
    " * The larger the dictionary the better\n",
    "* Non word spelling error correction:\n",
    " * Generate candidates : real words that are similar to error\n",
    " * Choose the one which is best method to solve such errors:\n",
    "   * Shortest weighted edit distance\n",
    "   * Highest noisy channel probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real word spelling errors\n",
    "\n",
    "* For each word w , generate candidate\n",
    " * Find candidate words with similar pronunciations\n",
    " * Find candidate words with similar spelling\n",
    " * Include w in candidate set\n",
    "* Choose best candidate to solve such errors:\n",
    " * Noisy Channel\n",
    " * Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Noisy Channel Model of Spelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images\Noisy_channel.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix a spelling we need candidates which can fix the spelling but that depends on the context hence:\n",
    "    \n",
    "Candidate generation\n",
    "* Words with similar spelling\n",
    " * Small edit distance to error\n",
    "* Words with similar pronunciation\n",
    " * Small edit distance of pronunciation to error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Damerau Levenshtein edit distance\n",
    "* Minimal edit distance between two strings, where edits are:\n",
    " * Insertion\n",
    " * Deletion\n",
    " * Substitution\n",
    " * Transposition of two adjacent letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example with word within 1 edit distance of acress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Error    |Candidate correction|Correct Letter|Error Letter|Type     |\n",
    "|:--------|:-------------------|:-------------|:-----------|:--------|\n",
    "|acress   |actress             |t             |-           |deletion |\n",
    "|acress   |cress               |-             |a           |insertion|\n",
    "|acress   |caress              |ca            |ac          |transposition|\n",
    "|acress   |access              |c             |r           |substitution|\n",
    "|acress   |across              |o             |e           |substitution|\n",
    "|acress   |acres               |-             |s           |insertion|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Candidate generation__\n",
    "\n",
    "* 80% of errors are within edit distance 1\n",
    "* Almost all errors within edit distance 2\n",
    "* Also allow insertion of space or hyphen\n",
    " * thisidea -> this idea\n",
    " * inlaw -> in law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Unigram Prior probability__\n",
    "\n",
    "Counts from 404,253,213 words in Corpus of Contemporary English (COCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"unigram_prior_prob.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the P(word) we can see that across seems to be the highest probablistic word to be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While using Bigrams we introduce probaiality of bigrams into the context which helps us decipher the error in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at an example:\n",
    "\n",
    "Using a bigram language model\n",
    "* \"a stellar and versatile __acress__ whose combination of sass and glamour…\n",
    "* Counts from the Corpus of Contemporary American English with add-1 smoothing\n",
    "\n",
    "\n",
    "* P(actress|versatile)=.000021 P(whose|actress) = .0010\n",
    "* P(across|versatile )=.000021 P(whose|across) = .000006\n",
    "\n",
    "\n",
    "* P(“versatile actress whose”) = .000021*.0010   = 210 x $10^{-10}$\n",
    "* P(“versatile across whose ”) = .000021*.000006 = 1 x $10^{-10} $\n",
    "\n",
    "Hence actress seems 210 times more probable than across"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
