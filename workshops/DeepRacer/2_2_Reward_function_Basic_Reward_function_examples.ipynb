{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. AWS DeepRacer Basic Reward Function Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lists some examples of the AWS DeepRacer reward function.<br><br>\n",
    "__Topics__\n",
    "\n",
    "Example 1: Follow the Center Line in Time Trials\n",
    "\n",
    "Example 2: Stay Inside the Two Borders in Time Trials\n",
    "\n",
    "Example 3: Prevent Zig-Zag in Time Trials\n",
    "\n",
    "Example 4: Stay On One Lane without Crashing into Stationary Obstacles or Moving Vehicles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example 1: Follow the Center Line in Time Trials__\n",
    "\n",
    "This example determines how far away the agent is from the center line, and gives higher reward if it is closer to the center of the track, encouraging the agent to closely follow the center line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(params):\n",
    "    '''\n",
    "    Example of rewarding the agent to follow center line\n",
    "    '''\n",
    "    \n",
    "    # Read input parameters\n",
    "    track_width = params['track_width']\n",
    "    distance_from_center = params['distance_from_center']\n",
    "\n",
    "    # Calculate 3 markers that are increasingly further away from the center line\n",
    "    marker_1 = 0.1 * track_width\n",
    "    marker_2 = 0.25 * track_width\n",
    "    marker_3 = 0.5 * track_width\n",
    "\n",
    "    # Give higher reward if the car is closer to center line and vice versa\n",
    "    if distance_from_center <= marker_1:\n",
    "        reward = 1\n",
    "    elif distance_from_center <= marker_2:\n",
    "        reward = 0.5\n",
    "    elif distance_from_center <= marker_3:\n",
    "        reward = 0.1\n",
    "    else:\n",
    "        reward = 1e-3  # likely crashed/ close to off track\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example 2: Stay Inside the Two Borders in Time Trials__\n",
    "\n",
    "This example simply gives high rewards if the agent stays inside the borders, and let the agent figure out what is the best path to finish a lap. It is easy to program and understand, but likely takes longer to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(params):\n",
    "    '''\n",
    "    Example of rewarding the agent to stay inside the two borders of the track\n",
    "    '''\n",
    "    \n",
    "    # Read input parameters\n",
    "    all_wheels_on_track = params['all_wheels_on_track']\n",
    "    distance_from_center = params['distance_from_center']\n",
    "    track_width = params['track_width']\n",
    "    \n",
    "    # Give a very low reward by default\n",
    "    reward = 1e-3\n",
    "\n",
    "    # Give a high reward if no wheels go off the track and \n",
    "    # the car is somewhere in between the track borders \n",
    "    if all_wheels_on_track and (0.5*track_width - distance_from_center) >= 0.05:\n",
    "        reward = 1.0\n",
    "\n",
    "    # Always return a float value\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example 3: Prevent Zig-Zag in Time Trials__\n",
    "\n",
    "This example incentivizes the agent to follow the center line but penalizes with lower reward if it steers too much, which helps prevent zig-zag behavior. The agent learns to drive smoothly in the simulator and likely keeps the same behavior when deployed in the physical vehicle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(params):\n",
    "    '''\n",
    "    Example of penalize steering, which helps mitigate zig-zag behaviors\n",
    "    '''\n",
    "    \n",
    "    # Read input parameters\n",
    "    distance_from_center = params['distance_from_center']\n",
    "    track_width = params['track_width']\n",
    "    steering = abs(params['steering_angle']) # Only need the absolute steering angle\n",
    "\n",
    "    # Calculate 3 marks that are farther and father away from the center line\n",
    "    marker_1 = 0.1 * track_width\n",
    "    marker_2 = 0.25 * track_width\n",
    "    marker_3 = 0.5 * track_width\n",
    "\n",
    "    # Give higher reward if the car is closer to center line and vice versa\n",
    "    if distance_from_center <= marker_1:\n",
    "        reward = 1.0\n",
    "    elif distance_from_center <= marker_2:\n",
    "        reward = 0.5\n",
    "    elif distance_from_center <= marker_3:\n",
    "        reward = 0.1\n",
    "    else:\n",
    "        reward = 1e-3  # likely crashed/ close to off track\n",
    "\n",
    "    # Steering penality threshold, change the number based on your action space setting\n",
    "    ABS_STEERING_THRESHOLD = 15 \n",
    "\n",
    "    # Penalize reward if the car is steering too much\n",
    "    if steering > ABS_STEERING_THRESHOLD:\n",
    "        reward *= 0.8\n",
    "\n",
    "    return float(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, you design your reward function to act like an incentive plan.<br> \n",
    "For example, to keep the agent staying as close to the center line as possible, the reward function could return a reward of 1.0 if the vehicle is within 3cm from the center, a reward of 0.5 if the agent is within 10cm and a reward of 0.001 (stands for zero for practical purposes) otherwise.<br>\n",
    "\n",
    "You can customize your reward function with relevant input parameters passed into the reward function. However, you should understand that if your reward function includes details specific to the training track (such as the shape of the track), your model might learn a policy which does not generalize to other tracks.\n",
    "\n",
    "Note that the model training process will try to optimize average total reward. Without taking this into account, your model might learn a policy which maximizes time spent accumulating reward (for example, by driving slowly or zig-zagging)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example 4: Stay On One Lane without Crashing into Stationary Obstacles or Moving Vehicles__\n",
    "\n",
    "This reward function rewards the agent to stay between the track borders and penalizes the agent for getting too close to the next object in the front.<br> The agent can move from lane to lane to avoid crashes.<br> The total reward is a weighted sum of the reward and penalty.<br> The example gives more weight to the penalty term to focus more on safety by avoiding crashes.<br> You can play with different averaging weights to train the agent with different driving behaviors and to achieve different driving performances.<br><br>\n",
    "__Note__: This example is for the object avoidance race type and can be ignored for the time trail competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(params):\n",
    "    '''\n",
    "    Example of rewarding the agent to stay inside two borders\n",
    "    and penalizing getting too close to the objects in front\n",
    "    '''\n",
    "\n",
    "    all_wheels_on_track = params['all_wheels_on_track']\n",
    "    distance_from_center = params['distance_from_center']\n",
    "    track_width = params['track_width']\n",
    "    objects_distance = params['objects_distance']\n",
    "    _, next_object_index = params['closest_objects']\n",
    "    objects_left_of_center = params['objects_left_of_center']\n",
    "    is_left_of_center = params['is_left_of_center']\n",
    "\n",
    "    # Initialize reward with a small number but not zero\n",
    "    # because zero means off-track or crashed\n",
    "    reward = 1e-3\n",
    "\n",
    "    # Reward if the agent stays inside the two borders of the track\n",
    "    if all_wheels_on_track and (0.5 * track_width - distance_from_center) >= 0.05:\n",
    "        reward_lane = 1.0\n",
    "    else:\n",
    "        reward_lane = 1e-3\n",
    "\n",
    "    # Penalize if the agent is too close to the next object\n",
    "    reward_avoid = 1.0\n",
    "\n",
    "    # Distance to the next object\n",
    "    distance_closest_object = objects_distance[next_object_index]\n",
    "    # Decide if the agent and the next object is on the same lane\n",
    "    is_same_lane = objects_left_of_center[next_object_index] == is_left_of_center\n",
    "    \n",
    "    if is_same_lane:\n",
    "        if 0.5 <= distance_closest_object < 0.8: \n",
    "            reward_avoid *= 0.5\n",
    "        elif 0.3 <= distance_closest_object < 0.5:\n",
    "            reward_avoid *= 0.2\n",
    "        elif distance_closest_object < 0.3:\n",
    "            reward_avoid = 1e-3 # Likely crashed\n",
    "\n",
    "    # Calculate reward by putting different weights on \n",
    "    # the two aspects above\n",
    "    reward += 1.0 * reward_lane + 4.0 * reward_avoid\n",
    "\n",
    "    return reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
