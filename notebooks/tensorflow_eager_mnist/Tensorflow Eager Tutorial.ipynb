{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Tutorial on Tensorflow Eager execution\n",
    "\n",
    "\n",
    "TensorFlow eager execution is an new addition to Google's TensorFlow framework. TensorFlow is a widely used framework for training and deploying deep learning models. Many topics covered in this tutorial are from the official TensorFlow website itself, I have tried to add a few points here and there that I found along the way as I adapted to this nuances of the framework. Eager execution is contentious since, for now, it trades speed for convenience. For some this tutorial might be very useful and for others it might be a rehash of the TensorFlow documentation hence I have tried to add links to the source material where ever I could to make it worth your while. \n",
    "\n",
    ". Previously, TensorFlow utilized a graph approach to constructing deep learning models where a network graph had to be coded up and a session had to be created and run. While the session mode in TensorFlow provides well optimized tools the eager mode is more suited for quick prototyping. We will be using TensorFlow 2.0 below for all the examples. The eager execution mode is available from tensorFlow 1.7+.\n",
    "\n",
    "Let us look at a simple example. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T21:18:49.422382Z",
     "start_time": "2020-02-03T21:18:21.436387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# checking the version of tensorflow\n",
    "print(tf.__version__) \n",
    "\n",
    "\n",
    "# In TensorFlow 2.0 this is the way\n",
    "# to activate eager execution mode\n",
    "# and ONLY this way. \n",
    "tf.executing_eagerly()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T21:19:00.858322Z",
     "start_time": "2020-02-03T21:18:49.436628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op StridedSlice in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "<tf.Variable 'Variable:0' shape=(1, 1) dtype=float32, numpy=array([[1.]], dtype=float32)>\n",
      " \n",
      " [1.] \n"
     ]
    }
   ],
   "source": [
    "# check if code is using gpu \n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# suppose we want to print a variable\n",
    "dummy = tf.Variable([[1.0]])\n",
    "dummy_numpy = dummy[0].numpy()\n",
    "\n",
    "print(dummy)\n",
    "print(\" \\n {} \".format(dummy_numpy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above block of code multiple things are happening so lets go over. It is always important to check which device TensorFlow is using to execute your code. In our case, we have wanted to ensure that it will use a local GPU to perform the calculations hence we placed the line below identify which device was used.\n",
    "\n",
    "```python \n",
    "# check if code is using gpu \n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "```\n",
    "\n",
    "The result was \n",
    "```python \n",
    "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
    "\n",
    "```\n",
    "As you can see, in the end it states that GPU:0 was used hence we can be sure that the GPU was used for computation.\n",
    "\n",
    "If you want to force TensorFlow to use a certain device, it is possible to do so using \n",
    "\n",
    "```python\n",
    "\n",
    "with tf.device(\"CPU:0\"):\n",
    "\n",
    "```\n",
    "In the above code, any code within the with statement will be executed by the CPU device rather than the GPU device. \n",
    "\n",
    "Now onto the core difference. In TensorFlow's graph mode statements like \n",
    "\n",
    "```python\n",
    "dummy = tf.Variable([[1.0]])\n",
    "print(dummy)\n",
    "```\n",
    "would not be possible. One would require a session.run() command to execute the above statement. Eager execution makes it easy to output the value of the tensor and more over it allows us to convert a tensor to a numpy value and tensor values to numpy values. \n",
    "\n",
    "Take for example\n",
    "\n",
    "```python \n",
    "dummy_np = np.array([10])\n",
    "tensor = tf.multiply(dummy_np, 2)\n",
    "print(\"Value of tensor {} \\ntype of tensor {}\" .format(tensor, type(tensor))) \n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T21:19:00.934893Z",
     "start_time": "2020-02-03T21:19:00.879706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Value of tensor [20] \n",
      "type of tensor <class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "# execute the code below to see the result \n",
    "\n",
    "dummy_np = np.array([10])\n",
    "tensor = tf.multiply(dummy_np, 2)\n",
    "print(\"Value of tensor {} \\ntype of tensor {}\" .format(tensor, type(tensor))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Taking gradients \n",
    "\n",
    "One really cool feature of TensorFlow eager is taking gradients. It is covered [here](https://www.tensorflow.org/tutorials/customization/autodiff) in the documentation. I will give a couple of examples here. \n",
    "\n",
    "Suppose you want to take gradients of a function, as you will have to do for backpropagation. You can use the gradient tape method. Let us define the problem as - \n",
    "\n",
    "$$ f(x) = x^3 $$\n",
    "what is the derivative of this function at $x=2$. The way to do this is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T21:19:01.012935Z",
     "start_time": "2020-02-03T21:19:00.942897Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Value of function at x=2 is 12.0\n"
     ]
    }
   ],
   "source": [
    "# define the variable and value\n",
    "x = tf.Variable([[2.0]])\n",
    "\n",
    "# record the function \n",
    "with tf.GradientTape() as tape: \n",
    "    function = x*x*x\n",
    "\n",
    "# take the derivative at the given value\n",
    "gradient_value = tape.gradient(function, x)\n",
    "\n",
    "#print derivative value \n",
    "print(\"Value of function at x=2 is {}\".format(gradient_value.numpy()[0][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In line 2 we define the variable that we are interested in and we set a value for the variable by defining a TensorFlow variable. We then encompass the function in the gradient tape method. Next, we take the gradient with respect to the x variable. ```python gradient_value ``` holds a eager tensor that contains the the value of the derivative at $x=2$. We covert it to a numpy array and display it in the last step. \n",
    "\n",
    "Using this we method we can do things like chain rule which is a crucial technique used in backpropagation. \n",
    "\n",
    "Suppose we define a problem- \n",
    "\n",
    "$$ f(z) = \\dfrac{1}{1+ e^{-z}} $$\n",
    "\n",
    "where\n",
    "$$ z=  x + x^2 + x^3$$\n",
    "\n",
    "So if we want to take the derivative with respect to $x$ at $x=2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T21:19:01.120968Z",
     "start_time": "2020-02-03T21:19:01.017893Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Pow in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Exp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Neg in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RealDiv in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Shape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op BroadcastGradientArgs in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sum in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "delf/delx at x=2 is 1.4136010577203706e-05\n"
     ]
    }
   ],
   "source": [
    "# define the variable and value\n",
    "x = tf.Variable([[2.0]])\n",
    "\n",
    "# record the function \n",
    "with tf.GradientTape() as tape: \n",
    "    tape.watch(x)\n",
    "    z =  x + tf.pow(x,2) + tf.pow(x,3)\n",
    "    e_term =  -tf.exp(z)\n",
    "    function = 1.0/(1+e_term)\n",
    "                    \n",
    "# take the derivative at the given value\n",
    "delf_delx = tape.gradient(function, x)\n",
    "print(\"delf/delx at x=2 is {}\".format(delf_delx[0][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T22:39:59.057680Z",
     "start_time": "2020-02-01T22:39:59.040653Z"
    }
   },
   "source": [
    "We used numpy equivalent functions that are defined for tensorflow like power and exponent to write the function. Pretty cool. It is also possible to take higher order derivatives, all we need to do is run another tape over the existing one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T21:19:01.214202Z",
     "start_time": "2020-02-03T21:19:01.128513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op Tile in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "delf/delx at x=2 is 1.4136010577203706e-05\n",
      "delf2/del2x at x=2 is -0.0002286711533088237\n"
     ]
    }
   ],
   "source": [
    "# define the variable and value\n",
    "x = tf.Variable([[2.0]])\n",
    "\n",
    "# record the function \n",
    "with tf.GradientTape() as tape2:\n",
    "    with tf.GradientTape() as tape: \n",
    "        tape.watch(x)\n",
    "        z =  x + tf.pow(x,2) + tf.pow(x,3)\n",
    "        e_term =  -tf.exp(z)\n",
    "        function = 1.0/(1+e_term)\n",
    "                    \n",
    "# take the derivative at the given value\n",
    "    delf_delx = tape.gradient(function, x)\n",
    "second_order_derivative  =  tape2.gradient(delf_delx, x)\n",
    "\n",
    "print(\"delf/delx at x=2 is {}\".format(delf_delx[0][0]))\n",
    "print(\"delf2/del2x at x=2 is {}\".format(second_order_derivative[0][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager execution MNIST example\n",
    "\n",
    "Coding up MNIST is somewhat different from the graph method of TensorFlow. If you are unaware of what the MNIST dataset is, take a look at info about it [here](https://en.wikipedia.org/wiki/MNIST_database). \n",
    "\n",
    "First we need to download the dataset and access the data. The code in this section is from [this](https://www.tensorflow.org/tutorials/quickstart/advanced) section of the tensorflow tutorial. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T21:19:03.107855Z",
     "start_time": "2020-02-03T21:19:01.221816Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set shape (60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_test =  x_test / 255.0\n",
    "\n",
    "print(\"training set shape {}\".format(x_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the training and the testing set and divide it by 255 to ensure that the pixel values lie in between 0 and 1. \n",
    "We need to add a new channel dimension to define it as a tensor since we want the shape to of the form (28,28,1) for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T21:19:03.139230Z",
     "start_time": "2020-02-03T21:19:03.119277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set shape (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Add a channels dimension\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]\n",
    "\n",
    "print(\"training set shape {}\".format(x_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create shuffle and batch our training set and batch our test set. TensorFlow gives us a nice utility for this. Dataset module contains the shuffle and batch option. For instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T21:19:03.385475Z",
     "start_time": "2020-02-03T21:19:03.149272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op RangeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Equal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LogicalAnd in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Select in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AnonymousRandomSeedGenerator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ShuffleDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op BatchDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op OptimizeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MakeIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op IteratorGetNextSync in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "tf.Tensor([0 5], shape=(2,), dtype=int64)\n",
      "tf.Tensor([3 6], shape=(2,), dtype=int64)\n",
      "tf.Tensor([7 4], shape=(2,), dtype=int64)\n",
      "tf.Tensor([9 8], shape=(2,), dtype=int64)\n",
      "tf.Tensor([2 1], shape=(2,), dtype=int64)\n",
      "Executing op DeleteIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "shuffled_batched = tf.data.Dataset.range(10).shuffle(5,seed=1 ).batch(2)\n",
    "for values in shuffled_batched: \n",
    "    print(values)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We batched and shuffled numbers from 1 to 10 . Just make sure that you shuffle first, batch later.   You can set the seed value to ensure that the shuffle order is the same always, especially when you are prototyping. Similarly, we will shuffle and batch the training set and batch the test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T21:19:03.431968Z",
     "start_time": "2020-02-03T21:19:03.391326Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op TensorSliceDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ShuffleDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op BatchDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "# shuffle and batch the training set\n",
    "batch_size = 32 \n",
    "num_images = 50000\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(num_images).batch(batch_size)\n",
    "\n",
    "\n",
    "# batch the test set \n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Define a model\n",
    " Next we define the model. We import dense, flatten and convolutional layers from keras and the model utlilty from keras as well. The model is defined below\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T21:19:03.479998Z",
     "start_time": "2020-02-03T21:19:03.440151Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "class MyModel(Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = Conv2D(32, 3, activation='relu',padding='same' )\n",
    "        self.flatten = Flatten()\n",
    "        self.d1 = Dense(128, activation='relu')\n",
    "        self.d2 = Dense(10, activation='softmax')\n",
    "        self.conv1_save = []\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model, we have the init method that contains the initialized layered. Super allows us to access the keras Model class. The call function lets us define the order in which network will be called. The current network is a 2d convolutional network followed by a 128 node dense layer and 10 node softmax layer which provides the output for the model. \n",
    "The last line is where we create an instance of the class MyModel(). \n",
    "\n",
    "Here is a diagram of the network. \n",
    "![nn](neural_network.jpg)\n",
    "\n",
    "The network takes in an 28 X 28 image and outputs probabilities for each class from digits 0 to 9. \n",
    "### Loss and optimizer \n",
    "\n",
    "Next, we create a loss object and an optimizer object. We use a sparse categorical cross entropy object which allows us to use labels without one hot encoding. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T21:19:03.511078Z",
     "start_time": "2020-02-03T21:19:03.485998Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss function \n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define metrics \n",
    "Next we define the metrics used for evaluation. Since, we are using sparse categorical entropy as loss, Tensorflow provides the sparse categorical accuracy metric which calculates the accuracy without having to worry about converting the labels from one hot encoding to integer values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T21:19:03.605019Z",
     "start_time": "2020-02-03T21:19:03.518344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing step functions \n",
    "Next, we have the train step and test step function.  You will notice the ```python @tf.function``` decorator on top of the train and test step functions. This converts the eagar mode model to a graph and executes the functions. The apply gradients methods applies gradient to the network for backpropagation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T21:19:03.635806Z",
     "start_time": "2020-02-03T21:19:03.614019Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "    \n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    \n",
    "    # apply gradients\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    # calculate training loss \n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "    \n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "  \n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop \n",
    "We will train the for 10 epochs. In the below cell, we have the code from the TensorFlow tutorial where we first reset the metrics at beginning of every epoch and then we run the training and testing steps and print the output. The code is fairly self explanatory. Once the model has been trained we want to make predictions on it using a new image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-03T21:18:23.140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op OptimizeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op IteratorGetNextSync in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "WARNING:tensorflow:Layer my_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_764 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_train_step_936 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_test_step_4768 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_test_step_5468 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DivNoNan in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Epoch 1, Loss: 0.1402057260274887, Accuracy: 95.70500183105469, Test Loss: 0.061783697456121445, Test Accuracy: 97.79999542236328\n",
      "Epoch 2, Loss: 0.04289647191762924, Accuracy: 98.69833374023438, Test Loss: 0.05098425969481468, Test Accuracy: 98.29999542236328\n",
      "Epoch 3, Loss: 0.023158734664320946, Accuracy: 99.24333190917969, Test Loss: 0.06458336114883423, Test Accuracy: 98.00999450683594\n",
      "Epoch 4, Loss: 0.013151810504496098, Accuracy: 99.57833099365234, Test Loss: 0.05778563767671585, Test Accuracy: 98.4000015258789\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # reset metrics for train set\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    # rest metrics for test set\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    # run training step\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "    \n",
    "    # run test set \n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "\n",
    "    # print the epoch number, loss and various metrics\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch+1,\n",
    "                            train_loss.result(),\n",
    "                            train_accuracy.result()*100,\n",
    "                            test_loss.result(),\n",
    "                            test_accuracy.result()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer on single image\n",
    "\n",
    " To predict the label of a single image take the image and convert it to a tensor and pass it to the model. It will yield softmax values, since we have 10 possible outcomes, we take the index of the maximum value of the softmax output. This is the predicted label. Below you can see that in two cases the predicted and actual labels are the same. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-03T21:18:23.218Z"
    }
   },
   "outputs": [],
   "source": [
    "# choose a test set image \n",
    "test_image = tf.convert_to_tensor(x_test[100][ tf.newaxis, ...])\n",
    "\n",
    "# get softmax values from the model \n",
    "softmax_value = model(test_image).numpy()\n",
    "\n",
    "# get predicted label\n",
    "predicted_label = np.argmax(softmax_value)\n",
    "\n",
    "# get actual label\n",
    "actual_label = y_test[100]\n",
    "print(\"predicted value {}, actual label {}\".format(predicted_label, actual_label))\n",
    "\n",
    "# plot test image\n",
    "plt.imshow(x_test[100].reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-03T21:18:23.223Z"
    }
   },
   "outputs": [],
   "source": [
    "# choose a test set image \n",
    "test_image = tf.convert_to_tensor(x_test[200][ tf.newaxis, ...])\n",
    "\n",
    "# get softmax values from the model \n",
    "softmax_value = model(test_image).numpy()\n",
    "\n",
    "# get predicted label\n",
    "predicted_label = np.argmax(softmax_value)\n",
    "\n",
    "# get actual label\n",
    "actual_label = y_test[200]\n",
    "print(\"predicted value {}, actual label {}\".format(predicted_label, actual_label))\n",
    "\n",
    "# plot test image\n",
    "plt.imshow(x_test[200].reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T08:53:41.955648Z",
     "start_time": "2020-02-01T08:53:41.932703Z"
    }
   },
   "source": [
    "### Get weights in the network \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T02:00:26.717387Z",
     "start_time": "2020-02-02T02:00:26.563377Z"
    }
   },
   "source": [
    "Suppose you are interested in getting weights for each layer. We can identify each layer in the following way- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-03T21:18:23.340Z"
    }
   },
   "outputs": [],
   "source": [
    "# model weights \n",
    "model_weights = [x.numpy() for x  in model.weights]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 6 arrays in the model_weights list - \n",
    "\n",
    "1) 0 th array  32 filters, 3 X 3 size.  <br>\n",
    "2) 32  size array bias vector <br>\n",
    "3) 25088 X 128 size matrix. Weights between flatten and 128 unit dense layer. <br> \n",
    "4) 128 size array bias vector. <br>\n",
    "5) 128 by 10 size matrix. Weights between 128 unit dense and 10 unit dense layer. <br> \n",
    "6) 10 size bias vector. <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T01:48:59.873924Z",
     "start_time": "2020-02-03T01:48:59.862929Z"
    }
   },
   "source": [
    "![neural_weight](neural_weights_2.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the image above, the objects in the orange brackets are matrix multiplied together to yield the object to the right side of the the arrow. The test image and the first set of filters are multiplied to yield 32 feature maps of 28 X 28 size. We add a 32 element bias vector to these feature maps and run a Relu activation function to get the layer activations. The feature maps form a multidimensional matrix which flattens to 25088 elements (28 X 28 X 32 = 25088). The next set of brackets involve the product of the flattened layer and a weight matrix of size 25088 X 128 which converts the 25088 weight vector to a 128 element array, to this we add the 128 term bias vector. Again, we run Relu activation function on this array to get the activations for the 128 node dense layer. The final bracket is the product of the 128 node dense layer and a 128 x 10  size weight matrix. This yield a 10 element array to which we add the 10 element bias vector and run it through the Softmax function to get class probabilities for each digit.     \n",
    "\n",
    "In the code below, the relu_conv layer is converted from float 64 to float 32 to perform the m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-03T21:18:23.637Z"
    }
   },
   "outputs": [],
   "source": [
    "# convolution to flattened layer  \n",
    "convolution_result = tf.nn.convolution(test_image, model_weights[0], padding=\"SAME\")  + model_weights[1]\n",
    "flattened_conv = tf.reshape(tf.nn.relu(convolution_result),[-1])\n",
    "\n",
    "# flatten layer 28*28*32 = 25088\n",
    "relu_conv = tf.reshape(flattened_conv,[1,25088])\n",
    "relu_conv_float32 = tf.cast(relu_conv,\"float32\")\n",
    "\n",
    "# flattened layer to dense 1 \n",
    "weights_product_1 = tf.linalg.matmul(relu_conv_float32, model_weights[2]) + model_weights[3]\n",
    "dense1 = tf.nn.relu(weights_product_1)\n",
    "\n",
    "#dense 1 to dense 2 \n",
    "weights_product_2 = tf.linalg.matmul(dense1, model_weights[4]) +  model_weights[5]\n",
    "dense2 = tf.nn.softmax(weights_product_2)\n",
    "\n",
    "\n",
    "print(dense2)\n",
    "print(model(test_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T04:21:16.829853Z",
     "start_time": "2020-02-03T04:21:16.817897Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "You can see that both results are the same. This means that our set of operations is the same as what the model object does. One might wonder why we are doing this? A benefit is that we can calculate the output of intermediate layers, for example, we can find the value of the convolutional feature maps and  plot them. Hence, extracting weights is extremely useful for us. \n",
    "\n",
    "Another useful takeaway from this exercise is we now know that a neural network is really nothing more than a sequence of matrix multiplication. Once we had the weights, we could have easily converted the image to a numpy array and carried out a series of matrix multiplications and and acquired a prediction. We chose to use TensorFlow for running inference since it allows us to run inference with 1 line of code model(test_image). Regardless of the framework, be it TensorFlow, Pytorch, MXnet, chainer etc fundamentally we are performing a set of matrix operations get a prediction. If we understand the fundamental ideas than we do not have to be married to a framework. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading model\n",
    "\n",
    "I found that using saved_model.save and load gave me better results for saving and loading models. So below is an example of this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-03T21:18:23.756Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "tf.saved_model.save(model, \"test_model\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-03T21:18:23.760Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.saved_model.load(\"test_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T01:49:00.449578Z",
     "start_time": "2020-02-03T01:44:29.836Z"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this article we have covered some of the bare minmumum basics of Eager execution in TensorFlow. There are a lot more functionalities that Eager mode has that we have not covered and the TensorFlow documentation covers most of them. However, in my personal experience how the documentation states and how things work usually tends to differ a little bit. Either way, its a good starting place to learn. \n",
    "\n",
    "Finally, TensorFlow absorbed Keras as of TensorFlow 2.0. Keras is a high level framework for deep learning and is pretty easy to use. We will have some tutorials with Keras soon so keep an eye out for those as well. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
