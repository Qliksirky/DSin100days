{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last notebook focused on introducing the perceptron algorithm. While the perceptron is impractical to use in practice, the core idea of the perceptron algorithm carried over to other neural network algorithms .\n",
    "\n",
    "This notebook introduces the idea of the multi-layer perceptron and shows how adding hidden layers to the percepton makes alogrithm better. The notebook is going to be broken down into following way- \n",
    "\n",
    "1) The need for a multi-layer perceptron <br>\n",
    "2) Basic structure of a multi-layer perceptron <br>\n",
    "3) Components of a multi-layer perceptron <br>\n",
    "4) Description of weights <br> \n",
    "\n",
    "\n",
    "Unlike the last notebook, this notebook will focus more on the intuition on the multi-layer perceptron (we shall refer to this as MLP) without any practical examples (these will come in later notebooks). We need to understand many pieces of MLP before we can look at a good practical example and over the course of the next few notebooks we will go over those piece. \n",
    "\n",
    "## The need for a multi-layered perceptron\n",
    "\n",
    "The perceptron is an algorithm that takes each row of data from a dataset and performs a weighted sum on it and uses an activation function to assign a class to each row. The perceptron is a linear classifier; the decision boundary that it generates is a straight line that divdes the feature space into two regions. \n",
    "\n",
    "<p>\n",
    "    <img src=\"perceptron_plus_boundary.jpg\" width=800 alt>\n",
    "    <b>Figure 1</b>\n",
    "</p>\n",
    "\n",
    "The perceptron generates a linear decision boundary like the one shown in the bottom left panel of Figure 1. Frequently, this type of decision boundary is insufficent to classify dataset. Below are three examples where a linear decision boundary does not work. \n",
    "\n",
    "<p>\n",
    "    <img src=\"nonlinear_three.jpg\" width=1000 alt>\n",
    "    <b>Figure 2</b>\n",
    "</p>\n",
    "\n",
    "The three examples above depict plausible spread of data in real datasets. For the first case in figure 2, a circular decision boundary is required to seperate the two classes, in the second case, a linear boundary may work but the accuracy will be low due to  the data distribution. In the third case, two lines may be able to seperate out the two classes but might not provide good accuracy. Clearly in all three cases, a linear boundary will fail to acheive a good boundary of seperation for the two classes. How can one generate a decision boundary for such scenarios?\n",
    "\n",
    "## Basic structure of a multilayer perceptron\n",
    "\n",
    "This is done by extending the perceptron to have a \"hidden layer\" between the input and output layer of the perceptron. \n",
    "\n",
    " It it has been mathematically shown that by introducing a hidden layer, a neural network can approximate a wide variety of functions[1]. Adding a hidden layer introduces extra weight parameters in the neural network. Take a look at the figure below. \n",
    " \n",
    " <p>\n",
    "    <img src=\"mlp_simple.jpg\" width=800 alt>\n",
    "    <b>Figure 3</b>\n",
    "</p>\n",
    "\n",
    "In figure 3, a single hidden layer is added between the input layer and the output layer. Each line that connects two nodes is represented by weight parameter. Before adding the hidden layer, we had two weight parameters $\\text{w}_1$ and $\\text{w}_2$.\n",
    "Adding the hidden layer introduces four new weight parameters. Each new weight parameter is represented by the line that connects the input layer nodes to the hidden layer nodes. Each node, in the input layer is connected to all the nodes in the hidden layer or another way to look at it is to say that each node in the hidden layer takes information from each of input layer node. \n",
    "\n",
    "Similar to the Perceptron architecture, learning the weights means that the neural network has \"learned\" how to identify the two different classes in the dataset. The procedure is similar to the perceptron, we initalize the weights, we take a row of data and check the predict the network makes and adjust the weights according to the predictions. Rather than learning the three weights we were learning the last time(including the bias), now we will have to learn 3 +  4 (number of connections between input nodes and hidden layer nodes)+ 1 (new bias term) = 8  weights. \n",
    "\n",
    "The details of training a MLP are different from that of a single layer Perceptron. For this we need to understand each of the elements the MLP. These are- \n",
    "\n",
    "1) Weights <br>\n",
    "2) Activation funcitons  <br>\n",
    "3) Loss functions  <br>\n",
    "4) Gradient descent   <br>\n",
    "5) Backpropagation  <br>\n",
    "\n",
    "The addition of a hidden layer, and the potential of adding more hidden layers, means that we need to incorporate that information while defining weights and input parameters. The MLP can use many types of activation functions, we will introduce the sigmoid and the relu which are commonly used for classification and regression task. The results from the network are fed into a loss function that helps us quantify the error in result. The perceptron tool had a loss function too, we skipped it in the interest of keeping things simple. With MLP however, it is an essential component since there are multiple task dependent loss functions to choose from based on classification or regression. Once the loss has been calcuated, Gradient descent and backpropagation are then used to adjust the weights of the neural network to produce better results. Backpropagation was very crucial to training MLPs to the forefront since it leads to large improvement in prediction capibilites. \n",
    "\n",
    "Clearly, all of this is not possible in a single notebook hence the content will be divided into multiple notebooks. This notebook will cover part 1 - Weights. The second notebook will cover actiavtion functions and loss functions and the third notebook will be dedicated to gradient descent and backpropagation. In the fourth notebook we will, combine all these ideas and provide practical examples of training a multi-layer perceptron using the MNIST dataset. \n",
    "\n",
    "The first part we will tackle will be how do we define weights in a multi-layer perceptron. \n",
    "\n",
    "### Defining weights in an MLP \n",
    "\n",
    "When weights were defined for a Perceptron, things we straightforward. We had two input nodes and a bias term and an output node. There were three connections between the input layer and the output layer. Our weighted sum had three terms. One for each node of the input layer. One for each feature in our dataset and a bias term. Life was easy. Adding a hidden layer not only changes the number of weights but also the number of weighted sums that we must write down. \n",
    "\n",
    "In order to write down the weighted sums. Let us start by looking at the connections between the input layer and the hidden layer. We will call the input layer as layer 0 and the hidden layer as layer 1. We will define some notation to identify the weights as shown in figure 4-  \n",
    " \n",
    " <p>\n",
    "    <img src=\"weightdef.jpg\" width=800 alt>\n",
    "    <b>Figure 4</b>\n",
    "</p> \n",
    "\n",
    "It is useful to have an upper index, which refers to the highest layer number and two lower indices refering to the starting and ending node in each layer. Another example would be- \n",
    "\n",
    "$$ {\\text{w}^{1}}_{22} \\rightarrow \\text{A connection that connects the second node of layer 0 to the second node of layer 1}  $$ \n",
    "\n",
    "why did we define this complicated looking expression? We need to write down a weighted sum for each of the nodes in the hidden layer, and for that purpose we need to identify which weights are part of the weighted sum. For example, the weighted sum expression for the first node in the hidden layer becomes.\n",
    "\n",
    "$$ \\text{weighted sum for node 1} = \\text{w}^{1}_{11} \\times  \\text{feature 1} + \\text{w}^{1}_{21} \\times  \\text{feature 2}    $$\n",
    "\n",
    "In a similar vein to the weight, rather than writing down \"weighted sum for node 1 in layer 1\" we can write it in the following notation. \n",
    "\n",
    "$$  z^1_{1} \\rightarrow  \\text{weighted sum for node 1 for layer 1 } $$\n",
    "\n",
    "Since the hidden layer has 2 nodes, there are two weighted sums to write down.\n",
    "\n",
    "$$ z^1_{1} = \\text{w}^{1}_{11} \\times  \\text{feature 1} + \\text{w}^{1}_{21} \\times  \\text{feature 2}    $$\n",
    "<br>\n",
    "$$ z^1_{2} = \\text{w}^{1}_{12} \\times  \\text{feature 1} + \\text{w}^{1}_{22} \\times  \\text{feature 2}    $$\n",
    "\n",
    " If you every get confused about the upper and lower indicies then refer to figure 4 for clarification. It takes some practice but you will settle into the notation.\n",
    "\n",
    "Once the weighted sums have been calculated, we put them into the activation function just like we did for the perceptron. So we will define a new term that represent the ouput of this operation. \n",
    "\n",
    "$$ a^1_1 = \\text{activation function}\\big( z^1_{1} \\big) $$ \n",
    "\n",
    "So the term on the left represents the the result of using the activation function on the weighted sum. Similar to the weighted sum, the upper index represented the layer number and the lower number represents the node number in that layer. Figure 5 explains shows the similarites between the notation. \n",
    "\n",
    "<p>\n",
    "    <img src=\"z_a_notation.jpg\" width=800 alt>\n",
    "    <b>Figure 4</b>\n",
    "</p> \n",
    "\n",
    "\n",
    " # References\n",
    " [1] Kurt Hornik (1991) \"Approximation Capabilities of Multilayer Feedforward Networks\", Neural Networks, 4(2), 251–257. doi:10.1016/0893-6080(91)90009-T\n",
    " \n",
    " # Other resources\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
