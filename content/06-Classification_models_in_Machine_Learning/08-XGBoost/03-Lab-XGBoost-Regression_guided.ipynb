{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03-Lab - XGBoost Regression\n",
    "\n",
    "In this lab, you need to work with Students data and grades as labels for Portugese students. Your task is to fit an XGBoost Regressor, and predict the grades (G3) based on suitable input variables. You are required to do the data manipulation (converting text to numeric), train-test splits, etc to create the model.\n",
    "\n",
    "This lab has 3 parts. In part 1 you will convert categorical featues to dummy features, In part 2 you will train a XGBoost regressor and compare it to a svr. In the final part, you will find the optimal parameters for the XGBoost random forest regressor and compare the results \n",
    "\n",
    "Data is availabe in : https://raw.githubusercontent.com/colaberry/DSin100days/master/data/student-por.csv\n",
    "\n",
    "Description is available here : https://archive.ics.uci.edu/ml/datasets/Student+Performance#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T02:16:01.815255Z",
     "start_time": "2020-05-06T02:16:01.110167Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the library\n",
    "import pandas as pd\n",
    "\n",
    "students_por_url = \"https://raw.githubusercontent.com/colaberry/DSin100days/master/data/student-por.csv\"\n",
    "students = pd.read_csv(students_por_url,sep=';')\n",
    "students.head()\n",
    "\n",
    "#Write your code here or in other code cells down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Converting categorical features to dummy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T02:16:01.967048Z",
     "start_time": "2020-05-06T02:16:01.816285Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "data_set = students[['traveltime','studytime','G3', 'age', 'G1', 'G2']]\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "\n",
    "tt_dummy_data = encoder.fit_transform('####').toarray()\n",
    "column_names= ['tt_dummy_'+str(x) for x in range(0,tt_dummy_data.shape[1])]\n",
    "t_dummy = # we need to generate a dataframe with data= tt_dummy data and appropriate column names\n",
    "data_set = pd.concat([data_set,t_dummy], axis=1)\n",
    "\n",
    "\n",
    "st_dummy_data = # similar steps to the traveltime variable, get the encoder fit transform\n",
    "column_names= ['st_dummy_'+str(x) for x in range(0,st_dummy_data.shape[1])]\n",
    "s_dummy = # we need to generate a dataframe with data = s_dummy and appropriate column names\n",
    "data_set = # similar to above concat s_dummy to data_set \n",
    "\n",
    "data_set_copy = data_set.copy()\n",
    "data_set_copy = data_set_copy.drop(columns=['traveltime','studytime'])\n",
    "\n",
    "columns_list = # get a list of columns for data_set_copy \n",
    "data_set_copy_shape = # shape of data_set_copy \n",
    "print(\"columns in the new dataset are {}\\n\".format(columns_list))\n",
    "print(\"shape of the dataset is {}\".format(data_set_copy_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "columns in the new dataset are ['traveltime', 'studytime', 'G3', 'age', 'G1', 'G2', 'tt_dummy_0', 'tt_dummy_1', 'tt_dummy_2', 'tt_dummy_3', 'st_dummy_0', 'st_dummy_1', 'st_dummy_2', 'st_dummy_3']\n",
    "\n",
    "shape of the dataset is (649, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T02:16:01.978541Z",
     "start_time": "2020-05-06T02:16:01.968091Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "y = # your traget variable \n",
    "X = # your training features\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1)\n",
    "\n",
    "training_shape = X_train.shape\n",
    "print(\"training data shape {}\".format(training_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T01:41:34.363764Z",
     "start_time": "2020-05-06T01:41:34.353475Z"
    }
   },
   "source": [
    "training data shape (584, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training data shape (584, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Training a XGBoost Random forest regressor and comparing it to SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T02:16:19.052135Z",
     "start_time": "2020-05-06T02:16:18.698165Z"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRFRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "xrf_reg = XGBRFRegressor(n_estimators=500, max_depth=500, random_state=5)\n",
    "# train the regressor, run predict and calcuate the mean square error. 2 to 3 lines of code\n",
    "print(\"Mean squared error from training Xgboost Random forest algorithm is {}\".format(mse_xrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean squared error from training Xgboost Random forest algorithm is 2.197034125041056"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T02:16:20.853066Z",
     "start_time": "2020-05-06T02:16:20.826998Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svr = SVR(kernel='rbf')\n",
    "svr_pred = svr.fit(X_train, y_train).predict(X_test)\n",
    "mse_svr = mean_squared_error(svr_pred, y_test)\n",
    "print(\"Mean squared error from training support vector regressor algorithm is {}\".format(mse_svr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T01:42:30.632489Z",
     "start_time": "2020-05-06T01:42:30.622825Z"
    }
   },
   "source": [
    "Mean squared error from training support vector regressor algorithm is 2.438191127137287\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: These are unoptimized algorithms, as you will see below, when we optimize the RandomForest regressor we will see better results. The same applies to the support vector regressor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Find optimal parameters for XGboost Random Forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T02:16:27.986007Z",
     "start_time": "2020-05-06T02:16:22.272086Z"
    }
   },
   "outputs": [],
   "source": [
    "# Running the xgboost regressor over multiple depth sizes and estimators\n",
    "\n",
    "## number of estimators \n",
    "estimators_range = [x for x in range(0,1000,50)]\n",
    "estimators_range[0]= 1 \n",
    "mse_estimators = {}\n",
    "for x in estimators_range: \n",
    "    # add the code to train the xgboost random forest regressor \n",
    "    # and calculate the mean squared error\n",
    "    mse_estimators[x] = mse_val\n",
    "    \n",
    "print(\"length of mse_estimators {}\".format(len(mse_estimators)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T01:45:28.769935Z",
     "start_time": "2020-05-06T01:45:28.767520Z"
    }
   },
   "source": [
    "length of mse_estimators 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T02:16:28.977019Z",
     "start_time": "2020-05-06T02:16:28.964569Z"
    }
   },
   "outputs": [],
   "source": [
    "def key_from_value(dt, value): \n",
    "    key_index = list(dt.values()).index(value)\n",
    "    return list(dt.keys())[key_index]\n",
    "\n",
    "min_mse = # from mse_estimators get the min value \n",
    "\n",
    "# getting the key from a value\n",
    "# this function will give you the first key with the lowest value\n",
    "# note that there can be many keys with the lowest values. \n",
    "# we just care about the first value \n",
    "lowest_estimator = key_from_value(\"####\", \"####\") # what are going to be the input values for key_from_value? \n",
    "print(\"The optimal number of trees for training this dataset is {}\".format(lowest_estimator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal number of trees for training this dataset is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T02:16:31.770886Z",
     "start_time": "2020-05-06T02:16:31.675770Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(15,5))\n",
    "plt.xlabel(\"number of estimators\")\n",
    "plt.ylabel(\"MSE\")\n",
    "\n",
    "plt.plot(estimators_range, mse_estimators.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T02:16:16.130255Z",
     "start_time": "2020-05-06T02:16:08.529091Z"
    }
   },
   "outputs": [],
   "source": [
    "# depth size estimation \n",
    "depth_range = [x for x in range(0,100,5)]\n",
    "depth_range[0] = 1\n",
    "mse_depth = {}\n",
    "for x in depth_range: \n",
    "    # keep the same random state as above,\n",
    "    # as use the optimized number of estimators\n",
    "    \n",
    "    xrf_reg = XGBRFRegressor(\"####\") \n",
    "    xrf_pred = xrf_reg.fit(X_train, y_train).predict(X_test)\n",
    "    mse_val = mean_squared_error(xrf_pred, y_test)\n",
    "    mse_depth[x] = mse_val \n",
    "    \n",
    "print(\"length of mse_depth {}\".format(len(mse_depth)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "length of mse_depth 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T02:16:16.141504Z",
     "start_time": "2020-05-06T02:16:16.137836Z"
    }
   },
   "outputs": [],
   "source": [
    "min_mse = min(mse_depth.values())\n",
    "lowest_depth = key_from_value(mse_depth, min_mse)\n",
    "print(\"The optimal depth for a tree while training this dataset is {}\".format(lowest_depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T01:50:16.167929Z",
     "start_time": "2020-05-06T01:50:16.149479Z"
    }
   },
   "source": [
    "The optimal depth for a tree while training this dataset is 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T02:16:16.159987Z",
     "start_time": "2020-05-06T02:16:16.151795Z"
    }
   },
   "outputs": [],
   "source": [
    "# training with optimal parameters. Add optimal parameters to \n",
    "\n",
    "xrf_reg = XGBRFRegressor('####')\n",
    "xrf_pred = xrf_reg.fit(X_train, y_train).predict(X_test)\n",
    "mse_optimized_xrf = mean_squared_error(xrf_pred, y_test)\n",
    "print(\"Mean squared error from optimizing both parameters is {} vs {} \".format(mse_optimized_xrf,mse_xrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean squared error from optimizing both parameters is 1.8466487319574376 vs 2.1689669326031864 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
