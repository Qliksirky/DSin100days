{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02-Lab - XGBoost Classification\n",
    "\n",
    "In this lab, you will work with the Heart dataset to predict if a person has AHD or not. More over, you will compare xgboost and all the other tree based algorithms that we have learned so far. In the second part of the lab, you will generate the label map for xgboost.\n",
    "\n",
    "Data is availabe in : https://raw.githubusercontent.com/colaberry/DSin100days/master/data/Heart.csv\n",
    "\n",
    "\"Some of the data in this lab are taken from \"An Introduction to Statistical Learning, with applications in R\"  (Springer, 2013) from the authors: G. James, D. Witten,  T. Hastie and R. Tibshirani \" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:34:03.359390Z",
     "start_time": "2020-05-05T15:34:02.625946Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing pandas\n",
    "import pandas as pd\n",
    "\n",
    "heart = pd.read_csv('https://raw.githubusercontent.com/colaberry/DSin100days/master/data/Heart.csv', na_values='?').dropna()\n",
    "heart.info()\n",
    "heart.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:34:03.364255Z",
     "start_time": "2020-05-05T15:34:03.360548Z"
    }
   },
   "outputs": [],
   "source": [
    "# get dataset  \n",
    "data_set = heart[[\"Age\",\"MaxHR\",\"AHD\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:34:03.898393Z",
     "start_time": "2020-05-05T15:34:03.365578Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np \n",
    "\n",
    "labels = LabelEncoder().fit_transform(data_set[\"AHD\"].values) \n",
    "colors = ['yellow','black']\n",
    "cmap= ListedColormap(colors)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.xlabel('Age', fontsize=15)\n",
    "plt.ylabel('MaxHR', fontsize=15)\n",
    "plt.scatter(data_set['Age'].values, data_set['MaxHR'].values, c=labels, cmap=cmap )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:34:03.957448Z",
     "start_time": "2020-05-05T15:34:03.899476Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree, metrics\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X = data_set[['Age','MaxHR']].values\n",
    "y = labels.copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1)\n",
    "print(\"y value min and max are : {},{}\".format(min(y),max(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Comparing all the classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:34:04.321901Z",
     "start_time": "2020-05-05T15:34:03.958552Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb_clf = # from xgb get the xgb classifier to with parameters random state=12, max_depth=2 and n_estimators=100\n",
    "xgb_pred= xgb_clf.fit(X_train,y_train).predict(X_test)\n",
    "xgb_acc = metrics.accuracy_score(xgb_pred, y_test)\n",
    "print(\"Accuracy of the xgboost classifier on the test set {}\".format(xgb_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of the xgboost classifier on the test set 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost also has a random forest classifier. A good exercise is to compare all tree based classifiers- xgboost classifier, decision tree, boosted trees and random forest. We will be doing this below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:34:04.365414Z",
     "start_time": "2020-05-05T15:34:04.323419Z"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRFClassifier\n",
    "# get the xgboost randomforest algorithm with same parameters as the xgb classifier. \n",
    "# you will have to fit and train and predict as well. Approximately 3 to 4 lines of code\n",
    "print(\"Accuracy of the xgboost Random forest classifier on the test set {}\".format(round(xrf_acc,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:34:04.373156Z",
     "start_time": "2020-05-05T15:34:04.367040Z"
    }
   },
   "outputs": [],
   "source": [
    "dt_clf = tree.DecisionTreeClassifier()\n",
    "# Train a decision tree classifier. Make sure you name your variables apporpriately.\n",
    "# parameters are random_state=12 and max_depth=2. 3 to 4 lines of code \n",
    "print(\"Accuracy of the decision tree classifier on the test set {}\".format(round(dt_acc,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of the decision tree classifier on the test set 0.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:41:25.453897Z",
     "start_time": "2020-05-05T15:41:25.451021Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Train a Random Forest classifier. Make sure you name your variables apporpriately.\n",
    "# parameters are random_state=12, n_estimators= 100 and max_depth=2. 3 to 4 lines of code \n",
    "print(\"Accuracy of the random forest classifier on the test set {}\".format(round(rf_acc,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of the random forest classifier on the test set 0.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:34:04.597639Z",
     "start_time": "2020-05-05T15:34:04.513323Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Train a Random Forest classifier. Make sure you name your variables apporpriately.\n",
    "# parameters are random_state=12, n_estimators= 100, learning_rate=0.01 and max_depth=2. 3 to 4 lines of code \n",
    "print(\"Accuracy of the gradient boosting classifier on the test set {}\".format(round(gb_acc,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of the gradient boosting classifier on the test set 0.73\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately you should be able to get-\n",
    "\n",
    "\n",
    "| Tree method            | Accuracy |\n",
    "|------------------------|----------|\n",
    "| XGBoost(normal)        | 70%      |\n",
    "| Decision tree          | 73%      |\n",
    "| XGBoost(Random forest) | 77%      |\n",
    "| Random Forest          | 77%      |\n",
    "| Boosted trees          | 73%      |\n",
    "\n",
    "\n",
    "From the above table it is evident that the Random Forest classifier does the best in either the regular or boosted form. A caveat is that we did not not carry out parameter tuning. We fixed the parameters such as max depth and number of estimators. In a real use case, one would have do gridsearchCV or parameter search and identify the best parameters to train the models. Hence the above table should be taken with a grain of salt since this is an poorly optimized comparison. What we can see is that the the biggest differences lie between Random Forest methods and non random forest methods. Random Forests are highly effective predictors in most scenarios. \n",
    "\n",
    "Next lets look generating the label map for XGboost.\n",
    "\n",
    "## Part 2: Label map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:34:04.603197Z",
     "start_time": "2020-05-05T15:34:04.598737Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_3d(x,y,plot_step=0.01): \n",
    "   \n",
    "\n",
    "    x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
    "    y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "    return xx, yy \n",
    "\n",
    "def plot_contour(xx,yy,Z): \n",
    "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
    "    return cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:34:04.687235Z",
     "start_time": "2020-05-05T15:34:04.604537Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "xx, yy = to_3d(X,y)\n",
    "\n",
    "# we are going to generate the label map for xgb_classifier \n",
    "Z = xgb_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "cmap= ListedColormap(colors)\n",
    "\n",
    "_ = plot_contour('####') # what are going to be the inputs for this function\n",
    "plt.scatter(data_set[\"Age\"].values, data_set[\"MaxHR\"].values, c=labels,cmap=cmap )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:31:02.725323Z",
     "start_time": "2020-05-05T15:31:02.716815Z"
    }
   },
   "source": [
    "<img src=\"../../../images/xgb_label_map.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
